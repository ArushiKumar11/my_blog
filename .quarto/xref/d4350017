{"entries":[],"headings":["learning-transferable-visual-models-from-natural-language-supervision-clip-a-deep-dive","motivation","historical-context-and-connection-to-multimodal-learning","the-evolution-of-visual-representation-learning","early-explorations-of-language-vision-connections","clips-innovations","clip-model-approach-and-training","what-is-clip","contrastive-learning","architecture-of-clip","training-architecture","first-step-contrastive-pre-training","second-step-create-dataset-classifiers-from-label-text","third-step-zero-shot-classification","clip-applications","integration-of-nlp-and-image-processing-tasks","content-moderation","basis-for-other-models","key-learnings-from-clip","the-power-of-natural-language-supervision","contrastive-learning-as-an-efficient-pre-training-strategy","zero-shot-transfer-as-a-primary-capability","the-role-of-prompt-engineering","robustness-to-distribution-shift","advantages-of-clip-over-traditional-vision-models","deeper-understanding-of-images","improved-data-efficiency","better-generalization-and-interpretability","exploring-clip-defining-a-custom-clip-model","train-on-flickr30-dataset","defining-clip-loss","defining-projection-and-encoder-layers","define-class-for-custom-clip-model","train-custom-model-for-three-epochs","zero-shot-image-classification-using-clip","conclusion","visualizing-clips-embedding-space","conclusion-1","exploring-prompt-engineering","conclusion-2","examining-robustness-to-distribution-shift","conclusion-3","calculating-distance-between-text-embedding-and-image-embedding","using-clip-to-retrieve-image-from-database-guven-a-text-prompt","vqa-task","reflections","what-surprised-me","scope-for-improvement","references","primary-paper","resources-used-for-this-analysis"]}