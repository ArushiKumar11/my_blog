{"title":"Learning Transferable Visual Models From Natural Language Supervision (CLIP): A Deep Dive","markdown":{"headingText":"Learning Transferable Visual Models From Natural Language Supervision (CLIP): A Deep Dive","containsRefs":false,"markdown":"\n--- title: \"Arushi's MMdP2 Notebook\" description: \"CLIP exploration for MMdP2 project\" date: 2025-05-08 categories: [multimodal, CLIP, project] format: ipynb --- \n\n\n*Arushi Kumar, 220150032*\n\n## Motivation\n\nWhen OpenAI released CLIP (Contrastive Language-Image Pre-training) in 2021, it represented a fundamental shift in how we approach computer vision problems. Rather than training models on curated labeled datasets like ImageNet, CLIP demonstrated the power of learning from natural language supervision at scale.\n\nI chose to analyze this paper because of its impact on multimodal learning and its  approach to solving a core challenge in machine learning: the transferability of models across diverse tasks without task-specific fine-tuning. It laid the foundation for developing many other multimodal systems.\n\nAlso CLIP's zero-shot capabilities: The ability to perform well on previously unseen tasks without any additional training data. This approach not only scales more efficiently but also makes AI systems more adaptable and aligned with human intentions through natural language.\n\nAdditionally, CLIP's architecture serves as a foundational element for many subsequent advancements in multimodal AI, including DALL-E, Stable Diffusion,CLAP, Meta's ImageBind(which learns a joint embedding across six modalities – images, text, audio, depth, thermal, and IMU data)  and other models. Understanding CLIP is crucial for comprehending the current landscape of multimodal learning and generative AI.\n\n![](https://i.ytimg.com/vi/Oh6OX6XO29A/maxresdefault.jpg)\n\n\n\n## Historical Context and Connection to Multimodal Learning\n\nScalable pre-training methods that learn directly from web-scale text collections have revolutionized NLP, enabling models like GPT-3 to perform zero-shot transfer across diverse tasks. \nCLIP extends this paradigm to computer vision, challenging the standard practice of relying on crowd-labeled datasets like ImageNet by learning visual representations directly from natural language supervision on web-scale data\n\n### The Evolution of Visual Representation Learning\n\n\n1. **Traditional Supervised Learning (pre-2015)**: Models like AlexNet, VGG, and ResNet were trained on manually labeled datasets like ImageNet, requiring extensive human annotation and limiting models to fixed categories.\n\n2. **Self-Supervised Learning (2015-2020)**: Approaches like contrastive learning (SimCLR), masked image modeling, and rotation prediction attempted to learn representations without explicit labels, but still required task-specific fine-tuning for downstream tasks.\n\n3. **Weakly-Supervised Learning**: Methods like those used in BiT (Big Transfer) leveraged larger but noisier labeled datasets (JFT-300M) to improve transfer learning capabilities.\n\n### Early Explorations of Language-Vision Connections\n\nThe idea of using language to supervise visual learning wasn't entirely new when CLIP emerged:\n\n- In 1999, Mori et al. explored improving image retrieval by training models to predict nouns and adjectives in paired text documents.\n- Quattoni et al. (2007) demonstrated learning more data-efficient image representations by using text in captions.\n- Joulin et al. (2016) showed that CNNs trained to predict words in image captions could learn useful representations.\n- Visual N-Grams (Li et al., 2017) used text supervision and demonstrated limited zero-shot capabilities.\n\nHowever, these earlier approaches achieved limited success and couldn't match the performance of supervised learning on standard benchmarks. For instance, Li et al. (2017) reached only 11.5% accuracy on ImageNet in a zero-shot setting.\n\n\nThe emergence of transformer-based models in NLP (BERT, GPT) showed the power of pre-training on large-scale text corpora. This inspired approaches to adapt similar techniques to computer vision and multimodal tasks:\n\n- ViLBERT, LXMERT, and other vision-language models (2019-2020) combined transformers for both modalities but required fine-tuning for downstream tasks.\n- VirTex, ICMLM, and ConVIRT (2020) explored transformer-based language modeling and contrastive objectives for learning image representations.\n\n### CLIP's Innovations\n\nCLIP built upon these foundations but made several crucial innovations:\n\n1. **Scale**: Training on 400 million image-text pairs, dwarfing previous approaches.\n2. **Simplicity**: Using a straightforward contrastive learning objective rather than complex architectures or objectives.\n3. **Zero-Shot Design**: Explicitly optimizing for zero-shot transfer rather than treating it as a secondary capability.\n4. **Natural Language Interface**: Creating a flexible way to specify visual concepts through language prompts.\n\n\n\n## CLIP Model: Approach and Training?\n\n### What is CLIP?\nCLIP is a joint image and text embedding model trained using 400 million image and text pairs in a self supervised way. This means that it maps both text and images to the same embedding space. So, for example, an image of a dog and the sentence “an image of a dog” would end up having very similar embeddings and be close to each other in the vector space. \nThus it learns visual concepts from natural language supervision. Unlike traditional vision models that are trained on fixed label sets, CLIP learns to connect images and text in a shared embedding space, enabling flexible zero-shot prediction for any visual concept that can be expressed in language.\nThe model consists of two encoders:\n\n+ An image encoder that maps images to a feature space\n+ A text encoder that maps text descriptions to the same feature space\n\nBy mapping both modalities to the same space, CLIP can measure the similarity between any image and any text, allowing it to perform a wide range of tasks without task-specific training data.\n### Natural Language Supervision and Dataset Used\nCLIP learns from image captions and text descriptions rather than specific labels like \"dog\" or \"cat.\" This lets it learn from millions of images on the internet without needing people to manually label each one. The researchers created a new dataset called WebImageText (WIT) with 400 million image-text pairs from the internet. To ensure variety, they searched for images with text related to 500,000 different topics and included up to 20,000 examples for each topic.\n\n(![image.png](attachment:f6346747-6d57-4615-a523-8c9f4c0285e8.png))\n*Figure: CLIP is much more efficient at zero-shot transfer than image caption baselines. Transformer-based language models learn 3x slower than a baseline which predicts a bag-of-words encoding of text. Swapping to the contrastive objective used in CLIP further improves efficiency by 4x.*\n\n### Contrastive Learning\n\nContrastive learning is a technique used in machine learning, particularly in the field of unsupervised learning. Contrastive learning is a method where we teach an AI model to recognize similarities and differences of a large number of data points.\n\nWe have a main item (the “anchor sample”), a similar item (“positive”), and a different item (“negative sample”). The goal is to make the model understand that the anchor and the positive item are alike, so it brings them closer together in its mind while recognizing that the negative item is different and pushing it away.\nA similar or “positive” image or might be from the same category (e.g., dogs) as the main image or a modified version of it, whereas a “negative” image would be entirely different, typically from another category (e.g., cats).\n\n\n\n![image.png](attachment:13fcfa98-83ef-40eb-9512-12563b767764.png)\n\nCLIP employs contrastive learning to align images with their corresponding text descriptions. The key aspects of this approach include:\n\n+ Training objective: Given a batch of N (image, text) pairs, CLIP learns to identify which of the N² possible (image, text) pairings actually occurred.\n+ Similarity maximization: For real pairs, CLIP maximizes the cosine similarity between image and text embeddings.\n+ Negative sample contrast: Simultaneously, CLIP minimizes similarity between non-matching pairs (all other N²-N combinations).\n+ Symmetric loss function: CLIP uses a symmetric cross-entropy loss that treats both image-to-text and text-to-image prediction equally.\n\nPseudocode is as follows\n\n### Architecture of CLIP \nClIP uses two separate architectures as the backbone for encoding vision and text datasets:\n\n+ image_encoder: Represents the neural network architecture (e.g., ResNet or Vision Transformer) responsible for encoding images.\n+ text_encoder: Represents the neural network architecture (e.g., CBOW, BERT, or Text Transformer) responsible for encoding textual information.\n+ Shared Embedding Space: The two encoders produce embeddings in a shared vector space. These shared embedding spaces allow CLIP to compare text and image representations and learn their underlying relationships.\n\nThe original CLIP model was trained from scratch without initializing the image encoder and the text encoder with pre-trained weights due to the large volume of the dataset (400 million image-text pairs) that they used to train their CLIP model.\n\n### Training Architecture\n\n#### first step : Contrastive Pre-training\n![image.png](attachment:acedf1e9-b108-4571-b039-bd61562af39a.png)\nCLIP is pre-trained on a large-scale dataset of 400 million (image, text data) pairs collected from the internet. During pre-training, the model is presented with pairs of images and text captions.Thus shared latent space embeddings are created.\n\n#### second step :Create Dataset Classifiers from Label Text\nFor each image, multiple text descriptions are created, including the correct one and several incorrect ones. This creates a mix of positive samples (matching) and negative sample (mismatched) pairs. These descriptions are fed into the text encoder, generating class-specific embeddings.\nAnd then, Contrastive Loss Function is used\n\n#### third step: Zero shot Classification\nNow, the trained text encoder is used as a zero-shot classifier. With a new image, CLIP can make zero-shot predictions. This is done by passing it through the image encoder and the dataset classifier without fine-tuning.\n![image.png](attachment:5c7d7a1a-f477-4ab6-b85b-2ea48a529022.png)\n\n\n\n\n## CLIP Applications \n\n### Integration of NLP and image processing tasks:\nexample tasks include - Generating text descriptions for images,Classify images based on textual descriptions, Edit images based on textual prompts.\n![image.png](attachment:c8307a1f-5929-46e8-9268-d78db8bdf3a2.png)\n\n### Content Moderation\nCLIP can be used to moderate content on online platforms by analyzing images and accompanying text to identify and filter out inappropriate or harmful content.\n\n### Basis for other Models :\nConcept of CLIP, along with its techniques, extends beyond images and text to embrace other modalities. Netflix, in this[blog post](https://netflixtechblog.com/building-in-video-search-936766f0017c), trained a model by combining video and text modalities in the common embedding space to enhance search within video applications. Contrastive Language-Audio Pretraining (CLAP) is another model that integrates text and audio modalities within the same embedding space, making it valuable for improving search functionalities within audio applications.\n\n## Key Learnings from CLIP\n\n### 1. The Power of Natural Language Supervision\n\nCLIP demonstrates that natural language can serve as a rich, flexible form of supervision for visual models. The advantages include:\n\n- **Scalability**: Leveraging existing image-text pairs from the internet without requiring manual labels.\n- **Expressivity**: Capturing nuanced visual concepts beyond simple object categories.\n- **Task-Agnosticity**: Supporting a wide range of downstream tasks without specialized architectures.\n\n### 2. Contrastive Learning as an Efficient Pre-training Strategy\n\nThe paper provides evidence that contrastive learning between image and text embeddings is more computationally efficient than alternative approaches:\n\n- 4x more efficient than bag-of-words prediction.\n- 12x more efficient than transformer-based language modeling.\n\nThis efficiency allowed CLIP to scale to hundreds of millions of training examples with available compute resources.\n\n### 3. Zero-Shot Transfer as a Primary Capability\n\nRather than treating zero-shot transfer as a secondary capability, CLIP is explicitly designed to excel at it:\n\n- The contrastive pre-training can be viewed as optimizing performance on a proxy computer vision dataset with 32,768 randomly created classes defined via natural language.\n- Zero-shot classifiers are created by embedding class names or descriptions in the same space as images.\n\nThis approach as per the paper achieves impressive results across diverse tasks including OCR, action recognition, geo-localization, and many types of fine-grained classification.\n\n### 4. The Role of Prompt Engineering\n\nThe paper introduces the concept of \"prompt engineering\" for vision models - crafting text templates that help specify the context for classification:\n\n- Using templates like \"A photo of a {label}\" instead of just the label text improves performance.\n- Context-specific prompts (e.g., \"A photo of a {label}, a type of pet\" for pet classification) further boost accuracy.\n- Ensembling across multiple prompts provides additional gains (5% improvement on ImageNet).\n\nThis connection to similar techniques in language models like GPT-3 highlights the convergence of language and vision paradigms.\n\n### 5. Robustness to Distribution Shift\n\nOne of the most striking findings is that zero-shot CLIP models are significantly more robust to distribution shifts than traditional supervised models:\n\n- The gap between performance on ImageNet and on various distribution-shifted datasets (ImageNetV2, ImageNet Sketch, etc.) is reduced by up to 75%.\n- This suggests that task-agnostic pre-training may inherently lead to more robust visual representations.\n\n### Advantages of CLIP over Traditional Vision Models\n\n#### Deeper Understanding of Images\nWhile traditional vision models can identify objects in images, CLIP goes further by understanding relationships and context. It can not only recognize a child in a park but also infer activities and emotions like \"playing\" or \"having fun.\"\n\n#### Improved Data Efficiency\nTraditional vision models require massive labeled datasets that are expensive and time-consuming to create. CLIP learns from natural language descriptions paired with images, reducing the need for manual labeling and making it more efficient for specialized domains with limited data.\n\n#### Better Generalization and Interpretability\nCLIP's training on diverse image-text pairs helps it generalize to new scenarios without specific training. Its connection to language also improves explainability - instead of just classifying an image, it can express its understanding through text, making its reasoning more transparent to users.\n\n## Exploring CLIP: Defining a Custom CLIP Model\nWe define a custom clip model and train it on flickr30 dataset\n\nGiven the compute and training resource required we use pre trained resnet\n\n\n### train on flickr30 dataset\nhere we load dataset and create a dataloader\n\n### defining CLIP loss\n\n### defining projection and encoder layers\n\n### define class for custom clip model\n\n### train custom model for three epochs\n\n### Zero-Shot Image Classification using CLIP\n\nLet's demonstrate CLIP's zero-shot classification capabilities with a simple example:\n\n#### conclusion: \nit performs zero shot classification correctly\n\n### Visualizing CLIP's Embedding Space\n\nLet's explore CLIP's embedding space by visualizing the text embeddings for various categories:\n\n#### conclusion: \nnote how in the embedding space, items of similar kind are together. That is all vehicles are together, all food items are together and so on\n\n### Exploring Prompt Engineering\n\nLet's examine the impact of different prompt templates on CLIP's zero-shot performance:\n\n#### conclusion : \nCLIP finds Panda and Bear to be closeby in embedding space. Also additional information like in the wild doesnt seem to help CLIP possibly because the image as we see doesnt have \"wild\" or forest in background\n\n### Examining Robustness to Distribution Shift\n\nLet's visualize CLIP's robustness to distribution shift by comparing its performance on different image styles:\n\n#### conclusion:\nCLIP is able to work on images from different distribtuions ( like sketches/ photo/painting etc). For sketches we see it wrongly predicts pear instead\n\n### calculating distance between text embedding and image embedding\n\n### using CLIP to retrieve image from database guven a text prompt\n\n### VQA TASK \n\nhttps://github.com/ArushiKumar11/DA312_VQA_Methods/tree/main/medical_vqa\n\nan implemented project of mine (earlier course project) \nthat finetunes a CLIP Model trained on Medical data ( PubMedCLIP) on image scan dataset \n\n\n## Reflections\n\n### What Surprised Me\n\n1. **Efficiency of Contrastive Learning**: The paper's demonstration that contrastive learning is 4x more efficient than bag-of-words prediction and 12x more efficient than transformer language modeling was eye-opening. This efficiency gap explains why previous approaches with similar ideas but different training objectives couldn't scale effectively.\n\n2. **Robustness to Distribution Shift**: One of the most surprising findings was that zero-shot CLIP models significantly outperform supervised models in terms of robustness to distribution shifts as per the paper. This suggests that the typical approach of optimizing for a specific dataset might inherently lead to models that overfit to dataset-specific patterns rather than learning generalizable visual concepts.\n\n\n3. **Prompt Engineering's Impact**: The significant performance improvements achieved through simple prompt engineering techniques (up to 5% on ImageNet) demonstrate how crucial the interface between the model and the task specification is.\nAs the paper quotes - \"Another issue we encountered is that it’s relatively rare in\n our pre-training dataset for the text paired with the image\n to be just a single word. Usually the text is a full sentence\n describing the image in some way. To help bridge this\n distribution gap, we found that using the prompt template\n “A photo of a label.” tobeagooddefault that\n helps specify the text is about the content of the image. This\n often improves performance over the baseline of using only\n the label text.\"\n\n5. **Dataset Influence Over Architecture**: The paper suggests that the dataset's scale and quality had a much larger impact on performance than architectural choices. This challenges the common research focus on architecture design and suggests allocating more resources to data curation and scaling.\n\n### Scope for Improvement\n\n1. **Computational Efficiency**: Despite being more efficient than alternatives, CLIP still requires enormous computational resources. The paper estimates that a 1000x increase in compute would be needed for zero-shot CLIP to match state-of-the-art supervised models on all tasks. Developing more compute-efficient training methods would make this approach more accessible.\n\n2. **Few-Shot Performance**: While CLIP excels at zero-shot tasks, the transition to few-shot learning is somewhat counterintuitive. Adding just a few examples sometimes decreases performance relative to zero-shot predictions. Developing methods that better integrate prior knowledge (from zero-shot) with example-based learning could yield significant improvements.\n\n3. **Handling Abstract and Systematic Tasks**: CLIP struggles with abstract and systematic tasks like counting objects in an image. This suggests limitations in how well natural language supervision captures certain visual reasoning capabilities. Combining CLIP's approach with methods specifically designed for reasoning tasks might address this gap.\n\n4. **Data Diversity and Bias**: The web-scale training data inevitably contains biases present in internet text and images. While the paper acknowledges these issues, there's significant room for improvement in developing methods to identify and mitigate these biases during training or inference.\n\n5. **Integration with Video Understanding**: While CLIP shows strong performance on action recognition from single frames, extending its capabilities to understand temporal dynamics in videos would be a valuable improvement. This might involve adapting the contrastive learning objective to include temporal information.\n\n\n6. **Explainability**: Like many deep learning models, CLIP's decision-making process lacks transparency. Developing methods to explain CLIP's predictions would increase trust and enable more effective human-AI collaboration. Recent work on feature visualization and attribution methods for multimodal models represents a promising direction for making CLIP's decisions more interpretable.\n\n\n\n## References\n\n### Primary Paper\n- Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. *arXiv preprint arXiv:2103.00020*.\n\n### Resources Used for This Analysis\n- OpenAI's CLIP GitHub Repository: https://github.com/openai/CLIP\n- CLIP Paper Implementation in PyTorch: https://github.com/openai/CLIP/tree/main/clip\n- Hugging Face's CLIP Documentation: https://huggingface.co/docs/transformers/model_doc/clip\n- Berkeley AI Research Blog (for reference on blog style): https://bair.berkeley.edu/blog/\n- Other Blogs and Youtube Videos on CLIP Model for better understanding\n- Github Repos of Custom CLIP Model codes and codes on experiments on CLIP Model\n-  ChatGPT for grammar correction, language refinement, and writing assistance (helping to improve clarity and structure of my own analysis)\n\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"output-file":"arushi-mmdp2.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.30","theme":["cosmo","brand"],"title-block-banner":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}