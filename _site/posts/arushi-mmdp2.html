<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.30">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>arushi-mmdp2 – My Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../about.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-de070a7b0ab54f8780927367ac907214.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-dfb324f25d9b1687192fa8be62ac8f9c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">My Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../about.html" aria-current="page"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/ArushiKumar11"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../posts/arushi-mmdp2.html">Posts</a></li><li class="breadcrumb-item"><a href="../posts/arushi-mmdp2.html">Learning Transferable Visual Models From Natural Language Supervision (CLIP): A Deep Dive</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../posts/arushi-mmdp2.html">Posts</a></li><li class="breadcrumb-item"><a href="../posts/arushi-mmdp2.html">Learning Transferable Visual Models From Natural Language Supervision (CLIP): A Deep Dive</a></li></ol></nav></header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">myblog</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Posts</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../posts/arushi-mmdp2.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Learning Transferable Visual Models From Natural Language Supervision (CLIP): A Deep Dive</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>— title: “Arushi’s MMdP2 Notebook” description: “CLIP exploration for MMdP2 project” date: 2025-05-08 categories: [multimodal, CLIP, project] format: ipynb —</p>
<section id="learning-transferable-visual-models-from-natural-language-supervision-clip-a-deep-dive" class="level1">
<h1>Learning Transferable Visual Models From Natural Language Supervision (CLIP): A Deep Dive</h1>
<p><em>Arushi Kumar, 220150032</em></p>
<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>When OpenAI released CLIP (Contrastive Language-Image Pre-training) in 2021, it represented a fundamental shift in how we approach computer vision problems. Rather than training models on curated labeled datasets like ImageNet, CLIP demonstrated the power of learning from natural language supervision at scale.</p>
<p>I chose to analyze this paper because of its impact on multimodal learning and its approach to solving a core challenge in machine learning: the transferability of models across diverse tasks without task-specific fine-tuning. It laid the foundation for developing many other multimodal systems.</p>
<p>Also CLIP’s zero-shot capabilities: The ability to perform well on previously unseen tasks without any additional training data. This approach not only scales more efficiently but also makes AI systems more adaptable and aligned with human intentions through natural language.</p>
<p>Additionally, CLIP’s architecture serves as a foundational element for many subsequent advancements in multimodal AI, including DALL-E, Stable Diffusion,CLAP, Meta’s ImageBind(which learns a joint embedding across six modalities – images, text, audio, depth, thermal, and IMU data) and other models. Understanding CLIP is crucial for comprehending the current landscape of multimodal learning and generative AI.</p>
<p><img src="https://i.ytimg.com/vi/Oh6OX6XO29A/maxresdefault.jpg" class="img-fluid"></p>
</section>
<section id="historical-context-and-connection-to-multimodal-learning" class="level2">
<h2 class="anchored" data-anchor-id="historical-context-and-connection-to-multimodal-learning">Historical Context and Connection to Multimodal Learning</h2>
<p>Scalable pre-training methods that learn directly from web-scale text collections have revolutionized NLP, enabling models like GPT-3 to perform zero-shot transfer across diverse tasks. CLIP extends this paradigm to computer vision, challenging the standard practice of relying on crowd-labeled datasets like ImageNet by learning visual representations directly from natural language supervision on web-scale data</p>
<section id="the-evolution-of-visual-representation-learning" class="level3">
<h3 class="anchored" data-anchor-id="the-evolution-of-visual-representation-learning">The Evolution of Visual Representation Learning</h3>
<ol type="1">
<li><p><strong>Traditional Supervised Learning (pre-2015)</strong>: Models like AlexNet, VGG, and ResNet were trained on manually labeled datasets like ImageNet, requiring extensive human annotation and limiting models to fixed categories.</p></li>
<li><p><strong>Self-Supervised Learning (2015-2020)</strong>: Approaches like contrastive learning (SimCLR), masked image modeling, and rotation prediction attempted to learn representations without explicit labels, but still required task-specific fine-tuning for downstream tasks.</p></li>
<li><p><strong>Weakly-Supervised Learning</strong>: Methods like those used in BiT (Big Transfer) leveraged larger but noisier labeled datasets (JFT-300M) to improve transfer learning capabilities.</p></li>
</ol>
</section>
<section id="early-explorations-of-language-vision-connections" class="level3">
<h3 class="anchored" data-anchor-id="early-explorations-of-language-vision-connections">Early Explorations of Language-Vision Connections</h3>
<p>The idea of using language to supervise visual learning wasn’t entirely new when CLIP emerged:</p>
<ul>
<li>In 1999, Mori et al.&nbsp;explored improving image retrieval by training models to predict nouns and adjectives in paired text documents.</li>
<li>Quattoni et al.&nbsp;(2007) demonstrated learning more data-efficient image representations by using text in captions.</li>
<li>Joulin et al.&nbsp;(2016) showed that CNNs trained to predict words in image captions could learn useful representations.</li>
<li>Visual N-Grams (Li et al., 2017) used text supervision and demonstrated limited zero-shot capabilities.</li>
</ul>
<p>However, these earlier approaches achieved limited success and couldn’t match the performance of supervised learning on standard benchmarks. For instance, Li et al.&nbsp;(2017) reached only 11.5% accuracy on ImageNet in a zero-shot setting.</p>
<p>The emergence of transformer-based models in NLP (BERT, GPT) showed the power of pre-training on large-scale text corpora. This inspired approaches to adapt similar techniques to computer vision and multimodal tasks:</p>
<ul>
<li>ViLBERT, LXMERT, and other vision-language models (2019-2020) combined transformers for both modalities but required fine-tuning for downstream tasks.</li>
<li>VirTex, ICMLM, and ConVIRT (2020) explored transformer-based language modeling and contrastive objectives for learning image representations.</li>
</ul>
</section>
<section id="clips-innovations" class="level3">
<h3 class="anchored" data-anchor-id="clips-innovations">CLIP’s Innovations</h3>
<p>CLIP built upon these foundations but made several crucial innovations:</p>
<ol type="1">
<li><strong>Scale</strong>: Training on 400 million image-text pairs, dwarfing previous approaches.</li>
<li><strong>Simplicity</strong>: Using a straightforward contrastive learning objective rather than complex architectures or objectives.</li>
<li><strong>Zero-Shot Design</strong>: Explicitly optimizing for zero-shot transfer rather than treating it as a secondary capability.</li>
<li><strong>Natural Language Interface</strong>: Creating a flexible way to specify visual concepts through language prompts.</li>
</ol>
</section>
</section>
<section id="clip-model-approach-and-training" class="level2">
<h2 class="anchored" data-anchor-id="clip-model-approach-and-training">CLIP Model: Approach and Training?</h2>
<section id="what-is-clip" class="level3">
<h3 class="anchored" data-anchor-id="what-is-clip">What is CLIP?</h3>
<p>CLIP is a joint image and text embedding model trained using 400 million image and text pairs in a self supervised way. This means that it maps both text and images to the same embedding space. So, for example, an image of a dog and the sentence “an image of a dog” would end up having very similar embeddings and be close to each other in the vector space. Thus it learns visual concepts from natural language supervision. Unlike traditional vision models that are trained on fixed label sets, CLIP learns to connect images and text in a shared embedding space, enabling flexible zero-shot prediction for any visual concept that can be expressed in language. The model consists of two encoders:</p>
<ul>
<li>An image encoder that maps images to a feature space</li>
<li>A text encoder that maps text descriptions to the same feature space</li>
</ul>
<p>By mapping both modalities to the same space, CLIP can measure the similarity between any image and any text, allowing it to perform a wide range of tasks without task-specific training data. ### Natural Language Supervision and Dataset Used CLIP learns from image captions and text descriptions rather than specific labels like “dog” or “cat.” This lets it learn from millions of images on the internet without needing people to manually label each one. The researchers created a new dataset called WebImageText (WIT) with 400 million image-text pairs from the internet. To ensure variety, they searched for images with text related to 500,000 different topics and included up to 20,000 examples for each topic.</p>
<p>(<img src="arushi-mmdp2_files/figure-html/28cf9dc9-c6bd-4e3c-912e-40f4984feaa5-1-f6346747-6d57-4615-a523-8c9f4c0285e8.png" class="img-fluid" alt="image.png">) <em>Figure: CLIP is much more efficient at zero-shot transfer than image caption baselines. Transformer-based language models learn 3x slower than a baseline which predicts a bag-of-words encoding of text. Swapping to the contrastive objective used in CLIP further improves efficiency by 4x.</em></p>
</section>
<section id="contrastive-learning" class="level3">
<h3 class="anchored" data-anchor-id="contrastive-learning">Contrastive Learning</h3>
<p>Contrastive learning is a technique used in machine learning, particularly in the field of unsupervised learning. Contrastive learning is a method where we teach an AI model to recognize similarities and differences of a large number of data points.</p>
<p>We have a main item (the “anchor sample”), a similar item (“positive”), and a different item (“negative sample”). The goal is to make the model understand that the anchor and the positive item are alike, so it brings them closer together in its mind while recognizing that the negative item is different and pushing it away. A similar or “positive” image or might be from the same category (e.g., dogs) as the main image or a modified version of it, whereas a “negative” image would be entirely different, typically from another category (e.g., cats).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="arushi-mmdp2_files/figure-html/d74d934a-a213-4c10-a0bd-f697d176ef9a-1-13fcfa98-83ef-40eb-9512-12563b767764.png" class="img-fluid figure-img"></p>
<figcaption>image.png</figcaption>
</figure>
</div>
<p>CLIP employs contrastive learning to align images with their corresponding text descriptions. The key aspects of this approach include:</p>
<ul>
<li>Training objective: Given a batch of N (image, text) pairs, CLIP learns to identify which of the N² possible (image, text) pairings actually occurred.</li>
<li>Similarity maximization: For real pairs, CLIP maximizes the cosine similarity between image and text embeddings.</li>
<li>Negative sample contrast: Simultaneously, CLIP minimizes similarity between non-matching pairs (all other N²-N combinations).</li>
<li>Symmetric loss function: CLIP uses a symmetric cross-entropy loss that treats both image-to-text and text-to-image prediction equally.</li>
</ul>
<p>Pseudocode is as follows</p>
<div id="7e7dacbf-9785-4ff0-94b8-745023d53916" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-05-08T13:41:10.898688Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-05-08T13:41:10.898408Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-05-08T13:41:10.908833Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-05-08T13:41:10.908184Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-05-08T13:41:10.898664Z&quot;}}" data-trusted="true" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">'''</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># image_encoder - ResNet or Vision Transformer</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># text_encoder - CBOW or Text Transformer</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># I[n, h, w, c] - minibatch of aligned images</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># T[n, l] - minibatch of aligned texts</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># W_i[d_i, d_e] - learned proj of image to embed</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># W_t[d_t, d_e] - learned proj of text to embed</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># t - learned temperature parameter</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># extract feature representations of each modality</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">I_f = image_encoder(I) #[n, d_i]</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">T_f = text_encoder(T) #[n, d_t]</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># joint multimodal embedding [n, d_e]</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">I_e = l2_normalize(np.dot(I_f, W_i), axis=1)</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">T_e = l2_normalize(np.dot(T_f, W_t), axis=1)</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># scaled pairwise cosine similarities [n, n]</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co">logits = np.dot(I_e, T_e.T) * np.exp(t)</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># symmetric loss function</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co">labels = np.arange(n)</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co">loss_i = cross_entropy_loss(logits, labels, axis=0)</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co">loss_t = cross_entropy_loss(logits, labels, axis=1)</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co">loss = (loss_i + loss_t)/2</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co">'''</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>'\n# image_encoder - ResNet or Vision Transformer\n# text_encoder - CBOW or Text Transformer\n# I[n, h, w, c] - minibatch of aligned images\n# T[n, l] - minibatch of aligned texts\n# W_i[d_i, d_e] - learned proj of image to embed\n# W_t[d_t, d_e] - learned proj of text to embed\n# t - learned temperature parameter\n# extract feature representations of each modality\nI_f = image_encoder(I) #[n, d_i]\nT_f = text_encoder(T) #[n, d_t]\n# joint multimodal embedding [n, d_e]\nI_e = l2_normalize(np.dot(I_f, W_i), axis=1)\nT_e = l2_normalize(np.dot(T_f, W_t), axis=1)\n# scaled pairwise cosine similarities [n, n]\nlogits = np.dot(I_e, T_e.T) * np.exp(t)\n# symmetric loss function\nlabels = np.arange(n)\nloss_i = cross_entropy_loss(logits, labels, axis=0)\nloss_t = cross_entropy_loss(logits, labels, axis=1)\nloss = (loss_i + loss_t)/2\n'</code></pre>
</div>
</div>
</section>
<section id="architecture-of-clip" class="level3">
<h3 class="anchored" data-anchor-id="architecture-of-clip">Architecture of CLIP</h3>
<p>ClIP uses two separate architectures as the backbone for encoding vision and text datasets:</p>
<ul>
<li>image_encoder: Represents the neural network architecture (e.g., ResNet or Vision Transformer) responsible for encoding images.</li>
<li>text_encoder: Represents the neural network architecture (e.g., CBOW, BERT, or Text Transformer) responsible for encoding textual information.</li>
<li>Shared Embedding Space: The two encoders produce embeddings in a shared vector space. These shared embedding spaces allow CLIP to compare text and image representations and learn their underlying relationships.</li>
</ul>
<p>The original CLIP model was trained from scratch without initializing the image encoder and the text encoder with pre-trained weights due to the large volume of the dataset (400 million image-text pairs) that they used to train their CLIP model.</p>
</section>
<section id="training-architecture" class="level3">
<h3 class="anchored" data-anchor-id="training-architecture">Training Architecture</h3>
<section id="first-step-contrastive-pre-training" class="level4">
<h4 class="anchored" data-anchor-id="first-step-contrastive-pre-training">first step : Contrastive Pre-training</h4>
<p><img src="arushi-mmdp2_files/figure-html/fe40bd46-aed4-4443-97ee-453bd0fc6762-2-acedf1e9-b108-4571-b039-bd61562af39a.png" class="img-fluid" alt="image.png"> CLIP is pre-trained on a large-scale dataset of 400 million (image, text data) pairs collected from the internet. During pre-training, the model is presented with pairs of images and text captions.Thus shared latent space embeddings are created.</p>
</section>
<section id="second-step-create-dataset-classifiers-from-label-text" class="level4">
<h4 class="anchored" data-anchor-id="second-step-create-dataset-classifiers-from-label-text">second step :Create Dataset Classifiers from Label Text</h4>
<p>For each image, multiple text descriptions are created, including the correct one and several incorrect ones. This creates a mix of positive samples (matching) and negative sample (mismatched) pairs. These descriptions are fed into the text encoder, generating class-specific embeddings. And then, Contrastive Loss Function is used</p>
</section>
<section id="third-step-zero-shot-classification" class="level4">
<h4 class="anchored" data-anchor-id="third-step-zero-shot-classification">third step: Zero shot Classification</h4>
<p>Now, the trained text encoder is used as a zero-shot classifier. With a new image, CLIP can make zero-shot predictions. This is done by passing it through the image encoder and the dataset classifier without fine-tuning. <img src="arushi-mmdp2_files/figure-html/fe40bd46-aed4-4443-97ee-453bd0fc6762-1-5c7d7a1a-f477-4ab6-b85b-2ea48a529022.png" class="img-fluid" alt="image.png"></p>
</section>
</section>
</section>
<section id="clip-applications" class="level2">
<h2 class="anchored" data-anchor-id="clip-applications">CLIP Applications</h2>
<section id="integration-of-nlp-and-image-processing-tasks" class="level3">
<h3 class="anchored" data-anchor-id="integration-of-nlp-and-image-processing-tasks">Integration of NLP and image processing tasks:</h3>
<p>example tasks include - Generating text descriptions for images,Classify images based on textual descriptions, Edit images based on textual prompts. <img src="arushi-mmdp2_files/figure-html/44c56f02-b694-4986-8b1e-070e82a40675-1-c8307a1f-5929-46e8-9268-d78db8bdf3a2.png" class="img-fluid" alt="image.png"></p>
</section>
<section id="content-moderation" class="level3">
<h3 class="anchored" data-anchor-id="content-moderation">Content Moderation</h3>
<p>CLIP can be used to moderate content on online platforms by analyzing images and accompanying text to identify and filter out inappropriate or harmful content.</p>
</section>
<section id="basis-for-other-models" class="level3">
<h3 class="anchored" data-anchor-id="basis-for-other-models">Basis for other Models :</h3>
<p>Concept of CLIP, along with its techniques, extends beyond images and text to embrace other modalities. Netflix, in this<a href="https://netflixtechblog.com/building-in-video-search-936766f0017c">blog post</a>, trained a model by combining video and text modalities in the common embedding space to enhance search within video applications. Contrastive Language-Audio Pretraining (CLAP) is another model that integrates text and audio modalities within the same embedding space, making it valuable for improving search functionalities within audio applications.</p>
</section>
</section>
<section id="key-learnings-from-clip" class="level2">
<h2 class="anchored" data-anchor-id="key-learnings-from-clip">Key Learnings from CLIP</h2>
<section id="the-power-of-natural-language-supervision" class="level3">
<h3 class="anchored" data-anchor-id="the-power-of-natural-language-supervision">1. The Power of Natural Language Supervision</h3>
<p>CLIP demonstrates that natural language can serve as a rich, flexible form of supervision for visual models. The advantages include:</p>
<ul>
<li><strong>Scalability</strong>: Leveraging existing image-text pairs from the internet without requiring manual labels.</li>
<li><strong>Expressivity</strong>: Capturing nuanced visual concepts beyond simple object categories.</li>
<li><strong>Task-Agnosticity</strong>: Supporting a wide range of downstream tasks without specialized architectures.</li>
</ul>
</section>
<section id="contrastive-learning-as-an-efficient-pre-training-strategy" class="level3">
<h3 class="anchored" data-anchor-id="contrastive-learning-as-an-efficient-pre-training-strategy">2. Contrastive Learning as an Efficient Pre-training Strategy</h3>
<p>The paper provides evidence that contrastive learning between image and text embeddings is more computationally efficient than alternative approaches:</p>
<ul>
<li>4x more efficient than bag-of-words prediction.</li>
<li>12x more efficient than transformer-based language modeling.</li>
</ul>
<p>This efficiency allowed CLIP to scale to hundreds of millions of training examples with available compute resources.</p>
</section>
<section id="zero-shot-transfer-as-a-primary-capability" class="level3">
<h3 class="anchored" data-anchor-id="zero-shot-transfer-as-a-primary-capability">3. Zero-Shot Transfer as a Primary Capability</h3>
<p>Rather than treating zero-shot transfer as a secondary capability, CLIP is explicitly designed to excel at it:</p>
<ul>
<li>The contrastive pre-training can be viewed as optimizing performance on a proxy computer vision dataset with 32,768 randomly created classes defined via natural language.</li>
<li>Zero-shot classifiers are created by embedding class names or descriptions in the same space as images.</li>
</ul>
<p>This approach as per the paper achieves impressive results across diverse tasks including OCR, action recognition, geo-localization, and many types of fine-grained classification.</p>
</section>
<section id="the-role-of-prompt-engineering" class="level3">
<h3 class="anchored" data-anchor-id="the-role-of-prompt-engineering">4. The Role of Prompt Engineering</h3>
<p>The paper introduces the concept of “prompt engineering” for vision models - crafting text templates that help specify the context for classification:</p>
<ul>
<li>Using templates like “A photo of a {label}” instead of just the label text improves performance.</li>
<li>Context-specific prompts (e.g., “A photo of a {label}, a type of pet” for pet classification) further boost accuracy.</li>
<li>Ensembling across multiple prompts provides additional gains (5% improvement on ImageNet).</li>
</ul>
<p>This connection to similar techniques in language models like GPT-3 highlights the convergence of language and vision paradigms.</p>
</section>
<section id="robustness-to-distribution-shift" class="level3">
<h3 class="anchored" data-anchor-id="robustness-to-distribution-shift">5. Robustness to Distribution Shift</h3>
<p>One of the most striking findings is that zero-shot CLIP models are significantly more robust to distribution shifts than traditional supervised models:</p>
<ul>
<li>The gap between performance on ImageNet and on various distribution-shifted datasets (ImageNetV2, ImageNet Sketch, etc.) is reduced by up to 75%.</li>
<li>This suggests that task-agnostic pre-training may inherently lead to more robust visual representations.</li>
</ul>
</section>
<section id="advantages-of-clip-over-traditional-vision-models" class="level3">
<h3 class="anchored" data-anchor-id="advantages-of-clip-over-traditional-vision-models">Advantages of CLIP over Traditional Vision Models</h3>
<section id="deeper-understanding-of-images" class="level4">
<h4 class="anchored" data-anchor-id="deeper-understanding-of-images">Deeper Understanding of Images</h4>
<p>While traditional vision models can identify objects in images, CLIP goes further by understanding relationships and context. It can not only recognize a child in a park but also infer activities and emotions like “playing” or “having fun.”</p>
</section>
<section id="improved-data-efficiency" class="level4">
<h4 class="anchored" data-anchor-id="improved-data-efficiency">Improved Data Efficiency</h4>
<p>Traditional vision models require massive labeled datasets that are expensive and time-consuming to create. CLIP learns from natural language descriptions paired with images, reducing the need for manual labeling and making it more efficient for specialized domains with limited data.</p>
</section>
<section id="better-generalization-and-interpretability" class="level4">
<h4 class="anchored" data-anchor-id="better-generalization-and-interpretability">Better Generalization and Interpretability</h4>
<p>CLIP’s training on diverse image-text pairs helps it generalize to new scenarios without specific training. Its connection to language also improves explainability - instead of just classifying an image, it can express its understanding through text, making its reasoning more transparent to users.</p>
</section>
</section>
</section>
<section id="exploring-clip-defining-a-custom-clip-model" class="level2">
<h2 class="anchored" data-anchor-id="exploring-clip-defining-a-custom-clip-model">Exploring CLIP: Defining a Custom CLIP Model</h2>
<p>We define a custom clip model and train it on flickr30 dataset</p>
<p>Given the compute and training resource required we use pre trained resnet</p>
<div id="b05bad99-5a90-400b-b593-1a8c1180033d" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-05-08T13:41:10.910163Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-05-08T13:41:10.909822Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-05-08T13:41:14.023133Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-05-08T13:41:14.022381Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-05-08T13:41:10.910138Z&quot;}}" data-trusted="true" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install datasets</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)
Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)
Requirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)
Requirement already satisfied: pyarrow&gt;=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)
Requirement already satisfied: dill&lt;0.3.9,&gt;=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)
Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)
Requirement already satisfied: requests&gt;=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)
Requirement already satisfied: tqdm&gt;=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)
Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)
Requirement already satisfied: multiprocess&lt;0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)
Requirement already satisfied: fsspec&lt;=2024.12.0,&gt;=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]&lt;=2024.12.0,&gt;=2023.1.0-&gt;datasets) (2024.12.0)
Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)
Requirement already satisfied: huggingface-hub&gt;=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)
Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)
Requirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)
Requirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (2.6.1)
Requirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (1.3.2)
Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (25.3.0)
Requirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (1.5.0)
Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (6.2.0)
Requirement already satisfied: propcache&gt;=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (0.3.1)
Requirement already satisfied: yarl&lt;2.0,&gt;=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (1.19.0)
Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub&gt;=0.24.0-&gt;datasets) (4.13.1)
Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy&gt;=1.17-&gt;datasets) (1.3.8)
Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy&gt;=1.17-&gt;datasets) (1.2.4)
Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy&gt;=1.17-&gt;datasets) (0.1.1)
Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy&gt;=1.17-&gt;datasets) (2025.1.0)
Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy&gt;=1.17-&gt;datasets) (2022.1.0)
Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy&gt;=1.17-&gt;datasets) (2.4.1)
Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.32.2-&gt;datasets) (3.4.1)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.32.2-&gt;datasets) (3.10)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.32.2-&gt;datasets) (2.3.0)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.32.2-&gt;datasets) (2025.1.31)
Requirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas-&gt;datasets) (2.9.0.post0)
Requirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas-&gt;datasets) (2025.2)
Requirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas-&gt;datasets) (2025.2)
Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;datasets) (1.17.0)
Requirement already satisfied: intel-openmp&lt;2026,&gt;=2024 in /usr/local/lib/python3.11/dist-packages (from mkl-&gt;numpy&gt;=1.17-&gt;datasets) (2024.2.0)
Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl-&gt;numpy&gt;=1.17-&gt;datasets) (2022.1.0)
Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*-&gt;mkl-&gt;numpy&gt;=1.17-&gt;datasets) (1.2.0)
Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath-&gt;numpy&gt;=1.17-&gt;datasets) (2024.2.0)
Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp&lt;2026,&gt;=2024-&gt;mkl-&gt;numpy&gt;=1.17-&gt;datasets) (2024.2.0)</code></pre>
</div>
</div>
<section id="train-on-flickr30-dataset" class="level3">
<h3 class="anchored" data-anchor-id="train-on-flickr30-dataset">train on flickr30 dataset</h3>
<p>here we load dataset and create a dataloader</p>
<div id="707f7be8-8c8b-48e1-9405-4234afee1b04" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-05-08T13:41:14.024696Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-05-08T13:41:14.024157Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-05-08T13:41:22.165840Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-05-08T13:41:22.165250Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-05-08T13:41:14.024672Z&quot;}}" data-trusted="true" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a custom dataset class for Flickr30k</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Flickr30kDataset(torch.utils.data.Dataset):</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dataset <span class="op">=</span> load_dataset(<span class="st">"nlphuji/flickr30k"</span>, cache_dir<span class="op">=</span><span class="st">"./huggingface_data"</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>            transforms.Resize((<span class="dv">224</span>, <span class="dv">224</span>)),</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>            transforms.ToTensor(),</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cap_per_image <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.dataset.num_rows[<span class="st">"test"</span>] <span class="op">*</span> <span class="va">self</span>.cap_per_image</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        original_idx <span class="op">=</span> idx <span class="op">//</span> <span class="va">self</span>.cap_per_image</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> <span class="va">self</span>.dataset[<span class="st">"test"</span>][original_idx][<span class="st">"image"</span>].convert(<span class="st">"RGB"</span>)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> <span class="va">self</span>.transform(image)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        caption <span class="op">=</span> <span class="va">self</span>.dataset[<span class="st">"test"</span>][original_idx][<span class="st">"caption"</span>][idx <span class="op">%</span> <span class="va">self</span>.cap_per_image]</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {<span class="st">"image"</span>: image, <span class="st">"caption"</span>: caption}</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an instance of the custom dataset</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>flickr30k_custom_dataset <span class="op">=</span> Flickr30kDataset()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="f517153a-d0f7-4fb5-a785-4be296f16dac" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-05-08T13:41:22.168166Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-05-08T13:41:22.167726Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-05-08T13:41:22.173130Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-05-08T13:41:22.172337Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-05-08T13:41:22.168147Z&quot;}}" data-trusted="true" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dataclasses <span class="im">import</span> dataclass</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Config:</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Configuration class for the CLIP training script.</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    embed_dim: <span class="bu">int</span> <span class="op">=</span> <span class="dv">512</span>  <span class="co"># Embedding dimension</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    transformer_embed_dim: <span class="bu">int</span> <span class="op">=</span> <span class="dv">768</span>  <span class="co"># Transformer embedding dimension</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    max_len: <span class="bu">int</span> <span class="op">=</span> <span class="dv">32</span>  <span class="co"># Maximum text length</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    text_model: <span class="bu">str</span> <span class="op">=</span> <span class="st">"distilbert-base-multilingual-cased"</span>  <span class="co"># Text model name</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    epochs: <span class="bu">int</span> <span class="op">=</span> <span class="dv">5</span>  <span class="co"># Number of training epochs</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    batch_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">128</span>  <span class="co"># Batch size</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="c1bda729-7fa6-4605-8385-de3d447adab6" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-05-08T13:41:22.174149Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-05-08T13:41:22.173865Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-05-08T13:41:22.193353Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-05-08T13:41:22.192713Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-05-08T13:41:22.174125Z&quot;}}" data-trusted="true" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the DataLoader</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>clip_dataloader <span class="op">=</span> DataLoader(flickr30k_custom_dataset, batch_size<span class="op">=</span>Config.batch_size, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="7654b375-2f45-40f5-beec-9c3d6561be91" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-05-08T13:41:22.194308Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-05-08T13:41:22.194123Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-05-08T13:41:25.652179Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-05-08T13:41:25.649254Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-05-08T13:41:22.194294Z&quot;}}" data-trusted="true" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an iterator from the dataloader</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>data_iter <span class="op">=</span> <span class="bu">iter</span>(clip_dataloader)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Get one batch</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> <span class="bu">next</span>(data_iter)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> batch[<span class="st">"image"</span>][<span class="dv">0</span>]  <span class="co"># Assuming batch size is greater than 0</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>caption <span class="op">=</span> batch[<span class="st">"caption"</span>][<span class="dv">0</span>]</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the image tensor to a NumPy array and permute dimensions</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>image_np <span class="op">=</span> np.transpose(image.numpy(), (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>))</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the image and caption</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>plt.imshow(image_np)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f"Caption: </span><span class="sc">{</span>caption<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="arushi-mmdp2_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="ca518148-9cd7-4a3e-80e1-54e38bdc6cfb" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-05-08T13:41:25.653696Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-05-08T13:41:25.653392Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-05-08T13:41:25.665116Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-05-08T13:41:25.661755Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-05-08T13:41:25.653663Z&quot;}}" data-trusted="true" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"example of a caption in abatch:"</span>,  batch[<span class="st">"caption"</span>][<span class="dv">0</span>])</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"number of captions in each batch:"</span>,<span class="bu">len</span>(batch[<span class="st">"caption"</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>example of a caption in abatch: A man, trying to hold on, as he competes in a rodeo on horse back.
number of captions in each batch: 128</code></pre>
</div>
</div>
</section>
<section id="defining-clip-loss" class="level3">
<h3 class="anchored" data-anchor-id="defining-clip-loss">defining CLIP loss</h3>
<div id="bad64696-61e7-4322-b5ce-3f4007ce57a8" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-05-08T13:41:25.665993Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-05-08T13:41:25.665731Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-05-08T13:41:32.486668Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-05-08T13:41:32.485941Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-05-08T13:41:25.665963Z&quot;}}" data-trusted="true" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install git<span class="op">+</span>https:<span class="op">//</span>github.com<span class="op">/</span>openai<span class="op">/</span>CLIP.git <span class="op">--</span>quiet</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  Preparing metadata (setup.py) ... done</code></pre>
</div>
</div>
<div id="6b2713d8-0021-4908-b86b-3b97838d238e" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-05-08T13:41:32.490604Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-05-08T13:41:32.490353Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-05-08T13:41:32.542627Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-05-08T13:41:32.541705Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-05-08T13:41:32.490581Z&quot;}}" data-trusted="true" data-execution_count="9">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> CLIP_loss(logits: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Assuming n is the number of classes</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> logits.shape[<span class="dv">1</span>]</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create labels tensor</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> torch.arange(n).to(device)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate cross entropy losses along axis 0 and 1</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    loss_i <span class="op">=</span> F.cross_entropy(logits.transpose(<span class="dv">0</span>, <span class="dv">1</span>), labels, reduction<span class="op">=</span><span class="st">"mean"</span>)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    loss_t <span class="op">=</span> F.cross_entropy(logits, labels, reduction<span class="op">=</span><span class="st">"mean"</span>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the final loss</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> (loss_i <span class="op">+</span> loss_t) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> metrics(similarity: torch.Tensor):</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> torch.arange(<span class="bu">len</span>(similarity)).to(similarity.device)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    img2cap_match_idx <span class="op">=</span> similarity.argmax(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    cap2img_match_idx <span class="op">=</span> similarity.argmax(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>    img_acc <span class="op">=</span> (img2cap_match_idx <span class="op">==</span> y).<span class="bu">float</span>().mean()</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    cap_acc <span class="op">=</span> (cap2img_match_idx <span class="op">==</span> y).<span class="bu">float</span>().mean()</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> img_acc, cap_acc</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="2745a7b8-9e9f-43c5-80f1-cb83dd8436f0" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-05-08T13:41:32.543739Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-05-08T13:41:32.543516Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-05-08T13:41:38.635943Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-05-08T13:41:38.635169Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-05-08T13:41:32.543722Z&quot;}}" data-trusted="true" data-execution_count="10">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> clip</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>model, preprocess <span class="op">=</span> clip.load(<span class="st">"ViT-B/32"</span>, device<span class="op">=</span>device)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> batch[<span class="st">"image"</span>].to(device)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>true_text <span class="op">=</span> batch[<span class="st">"caption"</span>]</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>wrong_text <span class="op">=</span> true_text[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> captions <span class="kw">in</span> [true_text, wrong_text]:</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> clip.tokenize(captions).to(device)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># with torch.no_grad():</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">=</span> model.encode_image(image)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> model.encode_text(text)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># normalized features</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">=</span> image_features <span class="op">/</span> image_features.norm(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> text_features <span class="op">/</span> text_features.norm(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    similarity <span class="op">=</span> text_features <span class="op">@</span> image_features.T</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> CLIP_loss(similarity)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(4.7188, device='cuda:0', dtype=torch.float16, grad_fn=&lt;DivBackward0&gt;)
tensor(4.8555, device='cuda:0', dtype=torch.float16, grad_fn=&lt;DivBackward0&gt;)</code></pre>
</div>
</div>
</section>
<section id="defining-projection-and-encoder-layers" class="level3">
<h3 class="anchored" data-anchor-id="defining-projection-and-encoder-layers">defining projection and encoder layers</h3>
<div id="4617cfc2-2540-4383-ae60-5e9416d6fc43" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-05-08T13:42:06.401150Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-05-08T13:42:06.400382Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-05-08T13:42:09.692111Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-05-08T13:42:09.691564Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-05-08T13:42:06.401122Z&quot;}}" data-trusted="true" data-execution_count="12">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModel, AutoTokenizer, BertTokenizer</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> models</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Projection(nn.Module):</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_in: <span class="bu">int</span>, d_out: <span class="bu">int</span>, p: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.5</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear1 <span class="op">=</span> nn.Linear(d_in, d_out, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear2 <span class="op">=</span> nn.Linear(d_out, d_out, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_norm <span class="op">=</span> nn.LayerNorm(d_out)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.drop <span class="op">=</span> nn.Dropout(p)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>        embed1 <span class="op">=</span> <span class="va">self</span>.linear1(x)</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>        embed2 <span class="op">=</span> <span class="va">self</span>.drop(<span class="va">self</span>.linear2(F.gelu(embed1)))</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        embeds <span class="op">=</span> <span class="va">self</span>.layer_norm(embed1 <span class="op">+</span> embed2)</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> embeds</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VisionEncoder(nn.Module):</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_out: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>        base <span class="op">=</span> models.resnet34(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>        d_in <span class="op">=</span> base.fc.in_features</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>        base.fc <span class="op">=</span> nn.Identity()</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.base <span class="op">=</span> base</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.projection <span class="op">=</span> Projection(d_in, d_out)</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.base.parameters():</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>            p.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>        projected_vec <span class="op">=</span> <span class="va">self</span>.projection(<span class="va">self</span>.base(x))</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>        projection_len <span class="op">=</span> torch.norm(projected_vec, dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> projected_vec <span class="op">/</span> projection_len</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TextEncoder(nn.Module):</span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_out: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.base <span class="op">=</span> AutoModel.from_pretrained(Config.text_model)</span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.projection <span class="op">=</span> Projection(Config.transformer_embed_dim, d_out)</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.base.parameters():</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>            p.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.base(x)[<span class="dv">0</span>]</span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> out[:, <span class="dv">0</span>, :]  <span class="co"># get CLS token output</span></span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a>        projected_vec <span class="op">=</span> <span class="va">self</span>.projection(out)</span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>        projection_len <span class="op">=</span> torch.norm(projected_vec, dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> projected_vec <span class="op">/</span> projection_len</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="93072bbb-3b1c-4536-99b0-890f96d278de" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-05-08T13:42:24.681406Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-05-08T13:42:24.680924Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-05-08T13:42:24.686064Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-05-08T13:42:24.685283Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-05-08T13:42:24.681383Z&quot;}}" data-trusted="true" data-execution_count="13">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Tokenizer:</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, tokenizer: BertTokenizer) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tokenizer <span class="op">=</span> tokenizer</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x: <span class="bu">str</span>) <span class="op">-&gt;</span> AutoTokenizer:</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.tokenizer(</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>            x, max_length<span class="op">=</span>Config.max_len, truncation<span class="op">=</span><span class="va">True</span>, padding<span class="op">=</span><span class="va">True</span>, return_tensors<span class="op">=</span><span class="st">"pt"</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>        )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="c33f161d-1f36-4cf8-a20a-8191c978ad68" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-05-08T13:42:39.905165Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-05-08T13:42:39.904487Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-05-08T13:43:08.343025Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-05-08T13:43:08.342263Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-05-08T13:42:39.905137Z&quot;}}" data-trusted="true" data-execution_count="14">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> batch[<span class="st">"image"</span>].to(device)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>true_text <span class="op">=</span> batch[<span class="st">"caption"</span>]</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>wrong_text <span class="op">=</span> true_text[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>vision_encoder <span class="op">=</span> VisionEncoder(Config.embed_dim).to(device)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>caption_encoder <span class="op">=</span> TextEncoder(Config.embed_dim).to(device)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> Tokenizer(AutoTokenizer.from_pretrained(Config.text_model))</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> captions <span class="kw">in</span> [true_text, wrong_text]:</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> tokenizer(captions).to(device)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># with torch.no_grad():</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">=</span> vision_encoder(image)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> caption_encoder(text[<span class="st">"input_ids"</span>])</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># normalized features</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">=</span> image_features <span class="op">/</span> image_features.norm(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> text_features <span class="op">/</span> text_features.norm(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>    similarity <span class="op">=</span> text_features <span class="op">@</span> image_features.T</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> CLIP_loss(similarity)</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Downloading: "https://download.pytorch.org/models/resnet34-b627a593.pth" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth
100%|██████████| 83.3M/83.3M [00:01&lt;00:00, 85.9MB/s]</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"062b8c14eab34932b6823dbfb729b832","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>2025-05-08 13:42:47.955754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746711768.127893     193 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746711768.178668     193 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"e7c53a19158a4dafb801712bffe30a72","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"0aad426fdfc745ccb759322aac31a309","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"66a0f294b2a547a5b1f0ee25b39d0e40","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"2e1db1bacaff4ef79390ac1feb23cc34","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(4.8544, device='cuda:0', grad_fn=&lt;DivBackward0&gt;)
tensor(4.8516, device='cuda:0', grad_fn=&lt;DivBackward0&gt;)</code></pre>
</div>
</div>
</section>
<section id="define-class-for-custom-clip-model" class="level3">
<h3 class="anchored" data-anchor-id="define-class-for-custom-clip-model">define class for custom clip model</h3>
<div id="69e6f9a1-c85e-487f-8fef-410efead702a" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-05-08T13:43:11.402806Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-05-08T13:43:11.402132Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-05-08T13:43:11.409597Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-05-08T13:43:11.408932Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-05-08T13:43:11.402781Z&quot;}}" data-trusted="true" data-execution_count="15">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> List, Tuple</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CustomModel(nn.Module):</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, lr: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1e-3</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vision_encoder <span class="op">=</span> VisionEncoder(Config.embed_dim)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.caption_encoder <span class="op">=</span> TextEncoder(Config.embed_dim)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tokenizer <span class="op">=</span> Tokenizer(AutoTokenizer.from_pretrained(Config.text_model, use_fast<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lr <span class="op">=</span> lr</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, images, text):</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> <span class="va">self</span>.tokenizer(text).to(<span class="va">self</span>.device)</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>        image_embed <span class="op">=</span> <span class="va">self</span>.vision_encoder(images)</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>        caption_embed <span class="op">=</span> <span class="va">self</span>.caption_encoder(text[<span class="st">"input_ids"</span>])</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>        similarity <span class="op">=</span> caption_embed <span class="op">@</span> image_embed.T</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> CLIP_loss(similarity)</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>        img_acc, cap_acc <span class="op">=</span> metrics(similarity)</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss, img_acc, cap_acc</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="9e422873-e5c2-4c41-9748-c9bbe13ec4f4" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-05-08T13:43:25.072412Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-05-08T13:43:25.071711Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-05-08T13:43:26.509740Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-05-08T13:43:26.509139Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-05-08T13:43:25.072380Z&quot;}}" data-trusted="true" data-execution_count="16">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an instance of your model</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> CustomModel().to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)</code></pre>
</div>
</div>
<div id="27a023de-1aab-4d8c-ba7f-bf11031e00ca" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-05-08T13:43:37.788859Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-05-08T13:43:37.788349Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-05-08T13:43:37.793734Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-05-08T13:43:37.793081Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-05-08T13:43:37.788833Z&quot;}}" data-trusted="true" data-execution_count="17">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define optimizer</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam([</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    {<span class="st">'params'</span>: model.vision_encoder.parameters()},</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    {<span class="st">'params'</span>: model.caption_encoder.parameters()}</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>], lr<span class="op">=</span>model.lr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="train-custom-model-for-three-epochs" class="level3">
<h3 class="anchored" data-anchor-id="train-custom-model-for-three-epochs">train custom model for three epochs</h3>
<div id="d28c203e-74ad-4217-a456-24a735367ee9" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-05-08T13:43:55.405562Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-05-08T13:43:55.405291Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-05-08T13:56:38.075410Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-05-08T13:56:38.074466Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-05-08T13:43:55.405542Z&quot;}}" data-trusted="true" data-execution_count="18">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>start_epoch <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>batch_zero <span class="op">=</span> <span class="va">True</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(start_epoch, num_epochs):</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> clip_dataloader:</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> batch[<span class="st">"image"</span>].to(device)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> batch[<span class="st">"caption"</span>]</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># images, text = batch</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>        loss, img_acc, cap_acc <span class="op">=</span> model(image, text)</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backward pass and optimization</span></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> batch_zero:</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>          <span class="bu">print</span>(<span class="ss">f"Epoch [</span><span class="sc">{</span><span class="dv">0</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>num_epochs<span class="sc">}</span><span class="ss">], Batch Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>          batch_zero <span class="op">=</span> <span class="va">False</span></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print training statistics</span></span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch [</span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>num_epochs<span class="sc">}</span><span class="ss">], Batch Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training complete."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch [0/3], Batch Loss: 4.852700233459473
Epoch [1/3], Batch Loss: 3.8830089569091797</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch [2/3], Batch Loss: 3.8700060844421387</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch [3/3], Batch Loss: 3.8735854625701904
Training complete.</code></pre>
</div>
</div>
</section>
<section id="zero-shot-image-classification-using-clip" class="level3">
<h3 class="anchored" data-anchor-id="zero-shot-image-classification-using-clip">Zero-Shot Image Classification using CLIP</h3>
<p>Let’s demonstrate CLIP’s zero-shot classification capabilities with a simple example:</p>
<div id="c1f35631-8123-4fb4-9fb4-61648ff7b384" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-05-08T14:00:31.227738Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-05-08T14:00:31.227028Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-05-08T14:00:34.380860Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-05-08T14:00:34.380108Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-05-08T14:00:31.227705Z&quot;}}" data-trusted="true" data-execution_count="19">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install required libraries</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install torch torchvision ftfy regex tqdm pillow matplotlib</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)
Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)
Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (6.3.1)
Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)
Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)
Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)
Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.5)
Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)
Requirement already satisfied: typing-extensions&gt;=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)
Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)
Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)
Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.12.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)
Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)
Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)
Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)
Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)
Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)
Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)
Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)
Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)
Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)
Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)
Requirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1-&gt;torch) (1.3.0)
Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)
Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)
Requirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)
Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)
Requirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)
Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)
Requirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)
Requirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)
Requirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)
Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy-&gt;torchvision) (1.3.8)
Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy-&gt;torchvision) (1.2.4)
Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy-&gt;torchvision) (0.1.1)
Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy-&gt;torchvision) (2025.1.0)
Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy-&gt;torchvision) (2022.1.0)
Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy-&gt;torchvision) (2.4.1)
Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.17.0)
Requirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2-&gt;torch) (3.0.2)
Requirement already satisfied: intel-openmp&lt;2026,&gt;=2024 in /usr/local/lib/python3.11/dist-packages (from mkl-&gt;numpy-&gt;torchvision) (2024.2.0)
Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl-&gt;numpy-&gt;torchvision) (2022.1.0)
Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*-&gt;mkl-&gt;numpy-&gt;torchvision) (1.2.0)
Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath-&gt;numpy-&gt;torchvision) (2024.2.0)
Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp&lt;2026,&gt;=2024-&gt;mkl-&gt;numpy-&gt;torchvision) (2024.2.0)</code></pre>
</div>
</div>
<div id="f22de53e-a132-49ad-9830-1471dfb12659" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-05-08T14:00:44.999553Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-05-08T14:00:44.999224Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-05-08T14:00:49.822618Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-05-08T14:00:49.822030Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-05-08T14:00:44.999528Z&quot;}}" data-trusted="true" data-execution_count="20">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import libraries</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> clip</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms <span class="im">import</span> Compose, Resize, CenterCrop, ToTensor, Normalize</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms.functional <span class="im">import</span> InterpolationMode</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the model</span></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>model, preprocess <span class="op">=</span> clip.load(<span class="st">"ViT-B/32"</span>, device<span class="op">=</span>device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="36955eb9-4cf5-47a3-ac6d-cbde102b7509" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-05-08T14:02:42.296057Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-05-08T14:02:42.295723Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-05-08T14:02:49.269334Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-05-08T14:02:49.268635Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-05-08T14:02:42.296036Z&quot;}}" data-trusted="true" data-execution_count="22">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Download an example image</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>url<span class="op">=</span><span class="st">"https://images.pexels.com/photos/1001976/pexels-photo-1001976.jpeg?cs=srgb&amp;dl=pexels-hemant-gupta-374105-1001976.jpg&amp;fm=jpg"</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="co">#url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Golden_Retriever_Hund_Dog.JPG/1280px-Golden_Retriever_Hund_Dog.JPG"</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the image</span></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare the image for CLIP</span></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>image_input <span class="op">=</span> preprocess(image).unsqueeze(<span class="dv">0</span>).to(device)</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>dog_breeds <span class="op">=</span> [</span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"golden retriever"</span>, <span class="st">"labrador retriever"</span>, <span class="st">"poodle"</span>, <span class="st">"german shepherd"</span>, </span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"bulldog"</span>, <span class="st">"beagle"</span>, <span class="st">"rottweiler"</span>, <span class="st">"siberian husky"</span>, <span class="st">"dachshund"</span></span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply prompt engineering</span></span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>text_inputs <span class="op">=</span> torch.cat([clip.tokenize(<span class="ss">f"a photo of a </span><span class="sc">{</span>breed<span class="sc">}</span><span class="ss">, a type of dog"</span>) <span class="cf">for</span> breed <span class="kw">in</span> dog_breeds]).to(device)</span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate features</span></span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">=</span> model.encode_image(image_input)</span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> model.encode_text(text_inputs)</span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Normalize features</span></span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">/=</span> image_features.norm(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">/=</span> text_features.norm(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-32"><a href="#cb38-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate similarity</span></span>
<span id="cb38-33"><a href="#cb38-33" aria-hidden="true" tabindex="-1"></a>    similarity <span class="op">=</span> (<span class="fl">100.0</span> <span class="op">*</span> image_features <span class="op">@</span> text_features.T).softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb38-34"><a href="#cb38-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-35"><a href="#cb38-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Print results</span></span>
<span id="cb38-36"><a href="#cb38-36" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, breed <span class="kw">in</span> <span class="bu">enumerate</span>(dog_breeds):</span>
<span id="cb38-37"><a href="#cb38-37" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>breed<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>similarity[<span class="dv">0</span>][i]<span class="sc">.</span>item()<span class="sc">:.2%}</span><span class="ss">"</span>)</span>
<span id="cb38-38"><a href="#cb38-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-39"><a href="#cb38-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize results</span></span>
<span id="cb38-40"><a href="#cb38-40" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb38-41"><a href="#cb38-41" aria-hidden="true" tabindex="-1"></a>plt.bar(dog_breeds, similarity[<span class="dv">0</span>].cpu().numpy() <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb38-42"><a href="#cb38-42" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Dog Breeds'</span>)</span>
<span id="cb38-43"><a href="#cb38-43" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Confidence (%)'</span>)</span>
<span id="cb38-44"><a href="#cb38-44" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'CLIP Zero-Shot Classification'</span>)</span>
<span id="cb38-45"><a href="#cb38-45" aria-hidden="true" tabindex="-1"></a>plt.xticks(rotation<span class="op">=</span><span class="dv">45</span>, ha<span class="op">=</span><span class="st">'right'</span>)</span>
<span id="cb38-46"><a href="#cb38-46" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb38-47"><a href="#cb38-47" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="arushi-mmdp2_files/figure-html/cell-21-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>golden retriever: 98.88%
labrador retriever: 1.03%
poodle: 0.03%
german shepherd: 0.00%
bulldog: 0.01%
beagle: 0.01%
rottweiler: 0.00%
siberian husky: 0.00%
dachshund: 0.03%</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="arushi-mmdp2_files/figure-html/cell-21-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="conclusion" class="level4">
<h4 class="anchored" data-anchor-id="conclusion">conclusion:</h4>
<p>it performs zero shot classification correctly</p>
</section>
</section>
<section id="visualizing-clips-embedding-space" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-clips-embedding-space">Visualizing CLIP’s Embedding Space</h3>
<p>Let’s explore CLIP’s embedding space by visualizing the text embeddings for various categories:</p>
<div id="17318de3-8a81-4fab-a267-b14d59a0e8b5" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-05-08T14:02:57.804376Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-05-08T14:02:57.803507Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-05-08T14:02:58.737400Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-05-08T14:02:58.736494Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-05-08T14:02:57.804345Z&quot;}}" data-trusted="true" data-execution_count="23">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define categories across different domains</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>categories <span class="op">=</span> {</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Animals'</span>: [<span class="st">'dog'</span>, <span class="st">'cat'</span>, <span class="st">'bird'</span>, <span class="st">'fish'</span>, <span class="st">'tiger'</span>, <span class="st">'elephant'</span>, <span class="st">'snake'</span>, <span class="st">'bear'</span>],</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Vehicles'</span>: [<span class="st">'car'</span>, <span class="st">'motorcycle'</span>, <span class="st">'bicycle'</span>, <span class="st">'bus'</span>, <span class="st">'train'</span>, <span class="st">'airplane'</span>, <span class="st">'boat'</span>, <span class="st">'helicopter'</span>],</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Food'</span>: [<span class="st">'pizza'</span>, <span class="st">'hamburger'</span>, <span class="st">'sushi'</span>, <span class="st">'salad'</span>, <span class="st">'cake'</span>, <span class="st">'ice cream'</span>, <span class="st">'apple'</span>, <span class="st">'banana'</span>],</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Clothing'</span>: [<span class="st">'shirt'</span>, <span class="st">'pants'</span>, <span class="st">'dress'</span>, <span class="st">'hat'</span>, <span class="st">'shoes'</span>, <span class="st">'jacket'</span>, <span class="st">'sweater'</span>, <span class="st">'socks'</span>]</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten the categories</span></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>all_categories <span class="op">=</span> [item <span class="cf">for</span> sublist <span class="kw">in</span> categories.values() <span class="cf">for</span> item <span class="kw">in</span> sublist]</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>domain_labels <span class="op">=</span> [domain <span class="cf">for</span> domain, items <span class="kw">in</span> categories.items() <span class="cf">for</span> _ <span class="kw">in</span> items]</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply prompt engineering</span></span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>prompts <span class="op">=</span> [<span class="ss">f"a photo of a </span><span class="sc">{</span>category<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> category <span class="kw">in</span> all_categories]</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>text_tokens <span class="op">=</span> clip.tokenize(prompts).to(device)</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Get text embeddings</span></span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> model.encode_text(text_tokens)</span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> text_features <span class="op">/</span> text_features.norm(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Reduce to 2D using t-SNE</span></span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a>tsne <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a>text_features_2d <span class="op">=</span> tsne.fit_transform(text_features.cpu().numpy())</span>
<span id="cb40-27"><a href="#cb40-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-28"><a href="#cb40-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a colormap for domains</span></span>
<span id="cb40-29"><a href="#cb40-29" aria-hidden="true" tabindex="-1"></a>domain_to_color <span class="op">=</span> {</span>
<span id="cb40-30"><a href="#cb40-30" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Animals'</span>: <span class="st">'red'</span>,</span>
<span id="cb40-31"><a href="#cb40-31" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Vehicles'</span>: <span class="st">'blue'</span>,</span>
<span id="cb40-32"><a href="#cb40-32" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Food'</span>: <span class="st">'green'</span>,</span>
<span id="cb40-33"><a href="#cb40-33" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Clothing'</span>: <span class="st">'purple'</span></span>
<span id="cb40-34"><a href="#cb40-34" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb40-35"><a href="#cb40-35" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [domain_to_color[domain] <span class="cf">for</span> domain <span class="kw">in</span> domain_labels]</span>
<span id="cb40-36"><a href="#cb40-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-37"><a href="#cb40-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb40-38"><a href="#cb40-38" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb40-39"><a href="#cb40-39" aria-hidden="true" tabindex="-1"></a>scatter <span class="op">=</span> plt.scatter(text_features_2d[:, <span class="dv">0</span>], text_features_2d[:, <span class="dv">1</span>], c<span class="op">=</span>colors, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb40-40"><a href="#cb40-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-41"><a href="#cb40-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Add labels</span></span>
<span id="cb40-42"><a href="#cb40-42" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, category <span class="kw">in</span> <span class="bu">enumerate</span>(all_categories):</span>
<span id="cb40-43"><a href="#cb40-43" aria-hidden="true" tabindex="-1"></a>    plt.annotate(category, (text_features_2d[i, <span class="dv">0</span>], text_features_2d[i, <span class="dv">1</span>]))</span>
<span id="cb40-44"><a href="#cb40-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-45"><a href="#cb40-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Add legend</span></span>
<span id="cb40-46"><a href="#cb40-46" aria-hidden="true" tabindex="-1"></a>legend_elements <span class="op">=</span> [plt.Line2D([<span class="dv">0</span>], [<span class="dv">0</span>], marker<span class="op">=</span><span class="st">'o'</span>, color<span class="op">=</span><span class="st">'w'</span>, </span>
<span id="cb40-47"><a href="#cb40-47" aria-hidden="true" tabindex="-1"></a>                             markerfacecolor<span class="op">=</span>color, markersize<span class="op">=</span><span class="dv">10</span>, label<span class="op">=</span>domain)</span>
<span id="cb40-48"><a href="#cb40-48" aria-hidden="true" tabindex="-1"></a>                  <span class="cf">for</span> domain, color <span class="kw">in</span> domain_to_color.items()]</span>
<span id="cb40-49"><a href="#cb40-49" aria-hidden="true" tabindex="-1"></a>plt.legend(handles<span class="op">=</span>legend_elements)</span>
<span id="cb40-50"><a href="#cb40-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-51"><a href="#cb40-51" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'t-SNE Visualization of CLIP Text Embeddings'</span>)</span>
<span id="cb40-52"><a href="#cb40-52" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb40-53"><a href="#cb40-53" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="arushi-mmdp2_files/figure-html/cell-22-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="conclusion-1" class="level4">
<h4 class="anchored" data-anchor-id="conclusion-1">conclusion:</h4>
<p>note how in the embedding space, items of similar kind are together. That is all vehicles are together, all food items are together and so on</p>
</section>
</section>
<section id="exploring-prompt-engineering" class="level3">
<h3 class="anchored" data-anchor-id="exploring-prompt-engineering">Exploring Prompt Engineering</h3>
<p>Let’s examine the impact of different prompt templates on CLIP’s zero-shot performance:</p>
<div id="a4797664-ce7d-408d-96b8-5a577660998f" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-05-08T15:33:14.814177Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-05-08T15:33:14.813864Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-05-08T15:33:15.638365Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-05-08T15:33:15.637632Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-05-08T15:33:14.814156Z&quot;}}" data-trusted="true" data-execution_count="52">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Giant_Panda_2004-03-2.jpg/1200px-Giant_Panda_2004-03-2.jpg"</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>image_input <span class="op">=</span> preprocess(image).unsqueeze(<span class="dv">0</span>).to(device)</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>animal_classes <span class="op">=</span> [<span class="st">"panda"</span>, <span class="st">"bear"</span>, <span class="st">"cat"</span>, <span class="st">"dog"</span>, <span class="st">"tiger"</span>]</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>prompt_templates <span class="op">=</span> [</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"</span><span class="sc">{}</span><span class="st">."</span>,  <span class="co"># Just the label</span></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"a photo of a </span><span class="sc">{}</span><span class="st">."</span>,  <span class="co"># Basic template</span></span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"a photo of a </span><span class="sc">{}</span><span class="st">, a type of animal."</span>,  <span class="co"># With category context</span></span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">"a close-up photo of a </span><span class="sc">{}</span><span class="st">."</span>,  <span class="co"># With visual context</span></span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">"a </span><span class="sc">{}</span><span class="st"> in the wild."</span>  <span class="co"># With environment context</span></span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> np.zeros((<span class="bu">len</span>(prompt_templates), <span class="bu">len</span>(animal_classes)))</span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, template <span class="kw">in</span> <span class="bu">enumerate</span>(prompt_templates):</span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a>    prompts <span class="op">=</span> [template.<span class="bu">format</span>(animal_class) <span class="cf">for</span> animal_class <span class="kw">in</span> animal_classes]</span>
<span id="cb41-26"><a href="#cb41-26" aria-hidden="true" tabindex="-1"></a>    text_tokens <span class="op">=</span> clip.tokenize(prompts).to(device)</span>
<span id="cb41-27"><a href="#cb41-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-28"><a href="#cb41-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb41-29"><a href="#cb41-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Encode the image and text</span></span>
<span id="cb41-30"><a href="#cb41-30" aria-hidden="true" tabindex="-1"></a>        image_features <span class="op">=</span> model.encode_image(image_input)</span>
<span id="cb41-31"><a href="#cb41-31" aria-hidden="true" tabindex="-1"></a>        text_features <span class="op">=</span> model.encode_text(text_tokens)</span>
<span id="cb41-32"><a href="#cb41-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb41-33"><a href="#cb41-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalize features</span></span>
<span id="cb41-34"><a href="#cb41-34" aria-hidden="true" tabindex="-1"></a>        image_features <span class="op">/=</span> image_features.norm(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb41-35"><a href="#cb41-35" aria-hidden="true" tabindex="-1"></a>        text_features <span class="op">/=</span> text_features.norm(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb41-36"><a href="#cb41-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb41-37"><a href="#cb41-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate similarity scores</span></span>
<span id="cb41-38"><a href="#cb41-38" aria-hidden="true" tabindex="-1"></a>        similarity <span class="op">=</span>  <span class="dv">100</span><span class="op">*</span>(image_features <span class="op">@</span> text_features.T).softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb41-39"><a href="#cb41-39" aria-hidden="true" tabindex="-1"></a>        results[i] <span class="op">=</span> similarity[<span class="dv">0</span>].cpu().numpy()</span>
<span id="cb41-40"><a href="#cb41-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Print similarity scores instead of bar chart</span></span>
<span id="cb41-41"><a href="#cb41-41" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, template <span class="kw">in</span> <span class="bu">enumerate</span>(prompt_templates):</span>
<span id="cb41-42"><a href="#cb41-42" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Prompt template: </span><span class="ch">\"</span><span class="sc">{</span>template<span class="sc">}</span><span class="ch">\"</span><span class="ss">"</span>)</span>
<span id="cb41-43"><a href="#cb41-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> cls, score <span class="kw">in</span> <span class="bu">zip</span>(animal_classes, results[i]):</span>
<span id="cb41-44"><a href="#cb41-44" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>cls<span class="sc">:&lt;10}</span><span class="ss">: </span><span class="sc">{</span>score<span class="sc">:.2f}</span><span class="ss">%"</span>)</span>
<span id="cb41-45"><a href="#cb41-45" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="arushi-mmdp2_files/figure-html/cell-23-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Prompt template: "{}."
  panda     : 21.48%
  bear      : 20.67%
  cat       : 18.78%
  dog       : 19.44%
  tiger     : 19.61%

Prompt template: "a photo of a {}."
  panda     : 21.25%
  bear      : 20.70%
  cat       : 19.06%
  dog       : 19.52%
  tiger     : 19.47%

Prompt template: "a photo of a {}, a type of animal."
  panda     : 21.34%
  bear      : 20.66%
  cat       : 19.11%
  dog       : 19.47%
  tiger     : 19.42%

Prompt template: "a close-up photo of a {}."
  panda     : 21.17%
  bear      : 20.75%
  cat       : 19.11%
  dog       : 19.56%
  tiger     : 19.39%

Prompt template: "a {} in the wild."
  panda     : 21.36%
  bear      : 20.56%
  cat       : 19.20%
  dog       : 19.47%
  tiger     : 19.41%</code></pre>
</div>
</div>
<section id="conclusion-2" class="level4">
<h4 class="anchored" data-anchor-id="conclusion-2">conclusion :</h4>
<p>CLIP finds Panda and Bear to be closeby in embedding space. Also additional information like in the wild doesnt seem to help CLIP possibly because the image as we see doesnt have “wild” or forest in background</p>
</section>
</section>
<section id="examining-robustness-to-distribution-shift" class="level3">
<h3 class="anchored" data-anchor-id="examining-robustness-to-distribution-shift">Examining Robustness to Distribution Shift</h3>
<p>Let’s visualize CLIP’s robustness to distribution shift by comparing its performance on different image styles:</p>
<div id="ad93211d-cc9e-4b6c-adff-f1c229da364c" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-05-08T15:31:44.049072Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-05-08T15:31:44.048670Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-05-08T15:31:45.434154Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-05-08T15:31:45.433316Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-05-08T15:31:44.049047Z&quot;}}" data-trusted="true" data-execution_count="51">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define image URLs for different styles of the same object</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>image_urls <span class="op">=</span> {</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Photo"</span>: <span class="st">"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Oranges_-_whole-halved-segment.jpg/1200px-Oranges_-_whole-halved-segment.jpg"</span>,</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Sketch"</span>: <span class="st">"https://i.ytimg.com/vi/5eOh2gFcXcQ/maxresdefault.jpg"</span>,</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Painting"</span>: <span class="st">"https://i.ytimg.com/vi/GJ0NlhSyG8A/maxresdefault.jpg"</span>,</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Cartoon"</span>: <span class="st">"https://www.shutterstock.com/image-vector/bright-vector-set-colorful-slice-260nw-604042622.jpg"</span></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>fruit_classes <span class="op">=</span> [<span class="st">"orange"</span>, <span class="st">"apple"</span>, <span class="st">"banana"</span>, <span class="st">"strawberry"</span>, <span class="st">"pear"</span>]</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>prompt_template <span class="op">=</span> <span class="st">"a photo of a </span><span class="sc">{}</span><span class="st">, a type of fruit."</span></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>text_inputs <span class="op">=</span> torch.cat([clip.tokenize(prompt_template.<span class="bu">format</span>(c)) <span class="cf">for</span> c <span class="kw">in</span> fruit_classes]).to(device)</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> model.encode_text(text_inputs)</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">/=</span> text_features.norm(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">10</span>))</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> axes.flatten()</span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> {}</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (style, url) <span class="kw">in</span> <span class="bu">enumerate</span>(image_urls.items()):</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load and display image</span></span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>    axes[i].imshow(image)</span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a>    axes[i].set_title(<span class="ss">f"Style: </span><span class="sc">{</span>style<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a>    axes[i].axis(<span class="st">'off'</span>)</span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a>    image_input <span class="op">=</span> preprocess(image).unsqueeze(<span class="dv">0</span>).to(device)</span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a>        image_features <span class="op">=</span> model.encode_image(image_input)</span>
<span id="cb43-33"><a href="#cb43-33" aria-hidden="true" tabindex="-1"></a>        image_features <span class="op">/=</span> image_features.norm(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb43-34"><a href="#cb43-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb43-35"><a href="#cb43-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate similarity</span></span>
<span id="cb43-36"><a href="#cb43-36" aria-hidden="true" tabindex="-1"></a>        similarity <span class="op">=</span> <span class="fl">100.0</span> <span class="op">*</span> (image_features <span class="op">@</span> text_features.T).softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb43-37"><a href="#cb43-37" aria-hidden="true" tabindex="-1"></a>        results[style] <span class="op">=</span> similarity[<span class="dv">0</span>].cpu().numpy()</span>
<span id="cb43-38"><a href="#cb43-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb43-39"><a href="#cb43-39" aria-hidden="true" tabindex="-1"></a>        prediction <span class="op">=</span> fruit_classes[similarity[<span class="dv">0</span>].argmax().item()]</span>
<span id="cb43-40"><a href="#cb43-40" aria-hidden="true" tabindex="-1"></a>        confidence <span class="op">=</span> similarity[<span class="dv">0</span>].<span class="bu">max</span>().item() </span>
<span id="cb43-41"><a href="#cb43-41" aria-hidden="true" tabindex="-1"></a>        axes[i].text(<span class="dv">10</span>, <span class="dv">30</span>, <span class="ss">f"Prediction: </span><span class="sc">{</span>prediction<span class="sc">}</span><span class="ch">\n</span><span class="ss">Confidence: </span><span class="sc">{</span>confidence<span class="sc">:.1f}</span><span class="ss">%"</span>, </span>
<span id="cb43-42"><a href="#cb43-42" aria-hidden="true" tabindex="-1"></a>                    bbox<span class="op">=</span><span class="bu">dict</span>(facecolor<span class="op">=</span><span class="st">'white'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>))</span>
<span id="cb43-43"><a href="#cb43-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Print confidence scores for all fruit classes</span></span>
<span id="cb43-44"><a href="#cb43-44" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> style, scores <span class="kw">in</span> results.items():</span>
<span id="cb43-45"><a href="#cb43-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Confidence scores for style: </span><span class="sc">{</span>style<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb43-46"><a href="#cb43-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> fruit, score <span class="kw">in</span> <span class="bu">zip</span>(fruit_classes, scores):</span>
<span id="cb43-47"><a href="#cb43-47" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>fruit<span class="sc">:&lt;10}</span><span class="ss">: </span><span class="sc">{</span>score<span class="sc">:.2f}</span><span class="ss">%"</span>)</span>
<span id="cb43-48"><a href="#cb43-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-49"><a href="#cb43-49" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb43-50"><a href="#cb43-50" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb43-51"><a href="#cb43-51" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Confidence scores for style: Photo
  orange    : 21.52%
  apple     : 19.88%
  banana    : 19.39%
  strawberry: 19.64%
  pear      : 19.58%

Confidence scores for style: Sketch
  orange    : 20.16%
  apple     : 20.22%
  banana    : 19.34%
  strawberry: 19.86%
  pear      : 20.44%

Confidence scores for style: Painting
  orange    : 21.09%
  apple     : 20.08%
  banana    : 19.44%
  strawberry: 19.70%
  pear      : 19.67%

Confidence scores for style: Cartoon
  orange    : 21.23%
  apple     : 19.89%
  banana    : 19.53%
  strawberry: 19.62%
  pear      : 19.72%</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="arushi-mmdp2_files/figure-html/cell-24-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="conclusion-3" class="level4">
<h4 class="anchored" data-anchor-id="conclusion-3">conclusion:</h4>
<p>CLIP is able to work on images from different distribtuions ( like sketches/ photo/painting etc). For sketches we see it wrongly predicts pear instead</p>
</section>
</section>
<section id="calculating-distance-between-text-embedding-and-image-embedding" class="level3">
<h3 class="anchored" data-anchor-id="calculating-distance-between-text-embedding-and-image-embedding">calculating distance between text embedding and image embedding</h3>
<div id="8e9b6aeb-46b0-4760-9cbd-78a8d9f2c3da" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-05-08T14:19:32.182325Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-05-08T14:19:32.182036Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-05-08T14:19:32.510394Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-05-08T14:19:32.509783Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-05-08T14:19:32.182306Z&quot;}}" data-trusted="true" data-execution_count="37">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load an image</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"https://cdn.shopify.com/s/files/1/0086/0795/7054/files/Golden-Retriever.jpg?v=1645179525"</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> requests.get(url)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> Image.<span class="bu">open</span>(BytesIO(response.content))</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the image</span></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>image_input <span class="op">=</span> preprocess(image).unsqueeze(<span class="dv">0</span>).to(device)</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Define classes to classify against</span></span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>classes <span class="op">=</span> [<span class="st">"a photo of a cat"</span>, <span class="st">"a photo of a dog"</span>, <span class="st">"a photo of a car"</span>, <span class="st">"a photo of a house"</span>]</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>text_inputs <span class="op">=</span> torch.cat([clip.tokenize(c) <span class="cf">for</span> c <span class="kw">in</span> classes]).to(device)</span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate features</span></span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">=</span> model.encode_image(image_input)</span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> model.encode_text(text_inputs)</span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalize features</span></span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a>image_features <span class="op">/=</span> image_features.norm(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">/=</span> text_features.norm(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb45-26"><a href="#cb45-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-27"><a href="#cb45-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate similarity</span></span>
<span id="cb45-28"><a href="#cb45-28" aria-hidden="true" tabindex="-1"></a>similarity <span class="op">=</span> (<span class="fl">100.0</span> <span class="op">*</span> image_features <span class="op">@</span> text_features.T).softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb45-29"><a href="#cb45-29" aria-hidden="true" tabindex="-1"></a>values, indices <span class="op">=</span> similarity[<span class="dv">0</span>].topk(<span class="bu">len</span>(classes))</span>
<span id="cb45-30"><a href="#cb45-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-31"><a href="#cb45-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Print results</span></span>
<span id="cb45-32"><a href="#cb45-32" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> value, index <span class="kw">in</span> <span class="bu">zip</span>(values, indices):</span>
<span id="cb45-33"><a href="#cb45-33" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>classes[index]<span class="sc">:&gt;16s}</span><span class="ss">: </span><span class="sc">{</span><span class="dv">100</span> <span class="op">*</span> value<span class="sc">.</span>item()<span class="sc">:.2f}</span><span class="ss">%"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="arushi-mmdp2_files/figure-html/cell-25-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>a photo of a dog: 99.85%
a photo of a cat: 0.15%
a photo of a house: 0.01%
a photo of a car: 0.00%</code></pre>
</div>
</div>
</section>
<section id="using-clip-to-retrieve-image-from-database-guven-a-text-prompt" class="level3">
<h3 class="anchored" data-anchor-id="using-clip-to-retrieve-image-from-database-guven-a-text-prompt">using CLIP to retrieve image from database guven a text prompt</h3>
<div id="eb9e6ca5-2fef-49d7-b1c4-aecd7d30e1a1" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-05-08T14:33:19.176532Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-05-08T14:33:19.175980Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-05-08T14:33:21.613360Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-05-08T14:33:21.612347Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-05-08T14:33:19.176510Z&quot;}}" data-trusted="true" data-execution_count="39">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to encode images from a folder</span></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> encode_image_folder(folder_path):</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>    encoded_images <span class="op">=</span> []</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>    image_paths <span class="op">=</span> []</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> filename <span class="kw">in</span> tqdm(os.listdir(folder_path)):</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> filename.lower().endswith((<span class="st">'.png'</span>, <span class="st">'.jpg'</span>, <span class="st">'.jpeg'</span>)):</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">try</span>:</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>                image_path <span class="op">=</span> os.path.join(folder_path, filename)</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>                image <span class="op">=</span> preprocess(Image.<span class="bu">open</span>(image_path)).unsqueeze(<span class="dv">0</span>).to(device)</span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>                <span class="cf">with</span> torch.no_grad():</span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a>                    image_features <span class="op">=</span> model.encode_image(image)</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a>                    image_features <span class="op">/=</span> image_features.norm(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a>                encoded_images.append(image_features.cpu().numpy())</span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a>                image_paths.append(image_path)</span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Error processing </span><span class="sc">{</span>filename<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb47-22"><a href="#cb47-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb47-23"><a href="#cb47-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.vstack(encoded_images), image_paths</span>
<span id="cb47-24"><a href="#cb47-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-25"><a href="#cb47-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Search images by text query</span></span>
<span id="cb47-26"><a href="#cb47-26" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> search_images(text_query, encoded_images, image_paths, top_k<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb47-27"><a href="#cb47-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb47-28"><a href="#cb47-28" aria-hidden="true" tabindex="-1"></a>        text_input <span class="op">=</span> clip.tokenize([text_query]).to(device)</span>
<span id="cb47-29"><a href="#cb47-29" aria-hidden="true" tabindex="-1"></a>        text_features <span class="op">=</span> model.encode_text(text_input)</span>
<span id="cb47-30"><a href="#cb47-30" aria-hidden="true" tabindex="-1"></a>        text_features <span class="op">/=</span> text_features.norm(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb47-31"><a href="#cb47-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb47-32"><a href="#cb47-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate similarities</span></span>
<span id="cb47-33"><a href="#cb47-33" aria-hidden="true" tabindex="-1"></a>    similarities <span class="op">=</span> (<span class="fl">100.0</span> <span class="op">*</span> torch.from_numpy(encoded_images) <span class="op">@</span> text_features.T.cpu()).numpy()</span>
<span id="cb47-34"><a href="#cb47-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb47-35"><a href="#cb47-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get top matches</span></span>
<span id="cb47-36"><a href="#cb47-36" aria-hidden="true" tabindex="-1"></a>    best_image_idx <span class="op">=</span> np.argsort(similarities.flatten())[::<span class="op">-</span><span class="dv">1</span>][:top_k]</span>
<span id="cb47-37"><a href="#cb47-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb47-38"><a href="#cb47-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [(image_paths[idx], similarities[idx][<span class="dv">0</span>]) <span class="cf">for</span> idx <span class="kw">in</span> best_image_idx]</span>
<span id="cb47-39"><a href="#cb47-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-40"><a href="#cb47-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Demo the image search</span></span>
<span id="cb47-41"><a href="#cb47-41" aria-hidden="true" tabindex="-1"></a>encoded_images, image_paths <span class="op">=</span> encode_image_folder(<span class="st">"/kaggle/input/image-folder/folder_with_images"</span>)</span>
<span id="cb47-42"><a href="#cb47-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-43"><a href="#cb47-43" aria-hidden="true" tabindex="-1"></a>queries <span class="op">=</span> [<span class="st">"a sunset over mountains"</span>, <span class="st">"a dog playing in the park"</span>, <span class="st">"food on a plate"</span>]</span>
<span id="cb47-44"><a href="#cb47-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-45"><a href="#cb47-45" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span> <span class="op">*</span> <span class="bu">len</span>(queries)))</span>
<span id="cb47-46"><a href="#cb47-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-47"><a href="#cb47-47" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, query <span class="kw">in</span> <span class="bu">enumerate</span>(queries):</span>
<span id="cb47-48"><a href="#cb47-48" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> search_images(query, encoded_images, image_paths)</span>
<span id="cb47-49"><a href="#cb47-49" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb47-50"><a href="#cb47-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j, (image_path, score) <span class="kw">in</span> <span class="bu">enumerate</span>(results):</span>
<span id="cb47-51"><a href="#cb47-51" aria-hidden="true" tabindex="-1"></a>        plt.subplot(<span class="bu">len</span>(queries), <span class="dv">5</span>, i <span class="op">*</span> <span class="dv">5</span> <span class="op">+</span> j <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb47-52"><a href="#cb47-52" aria-hidden="true" tabindex="-1"></a>        plt.imshow(Image.<span class="bu">open</span>(image_path))</span>
<span id="cb47-53"><a href="#cb47-53" aria-hidden="true" tabindex="-1"></a>        plt.axis(<span class="st">'off'</span>)</span>
<span id="cb47-54"><a href="#cb47-54" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="ss">f"Score: </span><span class="sc">{</span>score<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb47-55"><a href="#cb47-55" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb47-56"><a href="#cb47-56" aria-hidden="true" tabindex="-1"></a>    plt.figtext(<span class="fl">0.1</span>, <span class="fl">0.9</span> <span class="op">-</span> i <span class="op">*</span> <span class="fl">0.3</span>, <span class="ss">f"Query: '</span><span class="sc">{</span>query<span class="sc">}</span><span class="ss">'"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb47-57"><a href="#cb47-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-58"><a href="#cb47-58" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb47-59"><a href="#cb47-59" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">"image_search_results.png"</span>)</span>
<span id="cb47-60"><a href="#cb47-60" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb47-61"><a href="#cb47-61" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 7/7 [00:00&lt;00:00, 64.81it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="arushi-mmdp2_files/figure-html/cell-26-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="vqa-task" class="level3">
<h3 class="anchored" data-anchor-id="vqa-task">VQA TASK</h3>
<p>https://github.com/ArushiKumar11/DA312_VQA_Methods/tree/main/medical_vqa</p>
<p>an implemented project of mine (earlier course project) that finetunes a CLIP Model trained on Medical data ( PubMedCLIP) on image scan dataset</p>
</section>
</section>
<section id="reflections" class="level2">
<h2 class="anchored" data-anchor-id="reflections">Reflections</h2>
<section id="what-surprised-me" class="level3">
<h3 class="anchored" data-anchor-id="what-surprised-me">What Surprised Me</h3>
<ol type="1">
<li><p><strong>Efficiency of Contrastive Learning</strong>: The paper’s demonstration that contrastive learning is 4x more efficient than bag-of-words prediction and 12x more efficient than transformer language modeling was eye-opening. This efficiency gap explains why previous approaches with similar ideas but different training objectives couldn’t scale effectively.</p></li>
<li><p><strong>Robustness to Distribution Shift</strong>: One of the most surprising findings was that zero-shot CLIP models significantly outperform supervised models in terms of robustness to distribution shifts as per the paper. This suggests that the typical approach of optimizing for a specific dataset might inherently lead to models that overfit to dataset-specific patterns rather than learning generalizable visual concepts.</p></li>
<li><p><strong>Prompt Engineering’s Impact</strong>: The significant performance improvements achieved through simple prompt engineering techniques (up to 5% on ImageNet) demonstrate how crucial the interface between the model and the task specification is. As the paper quotes - “Another issue we encountered is that it’s relatively rare in our pre-training dataset for the text paired with the image to be just a single word. Usually the text is a full sentence describing the image in some way. To help bridge this distribution gap, we found that using the prompt template “A photo of a label.” tobeagooddefault that helps specify the text is about the content of the image. This often improves performance over the baseline of using only the label text.”</p></li>
<li><p><strong>Dataset Influence Over Architecture</strong>: The paper suggests that the dataset’s scale and quality had a much larger impact on performance than architectural choices. This challenges the common research focus on architecture design and suggests allocating more resources to data curation and scaling.</p></li>
</ol>
</section>
<section id="scope-for-improvement" class="level3">
<h3 class="anchored" data-anchor-id="scope-for-improvement">Scope for Improvement</h3>
<ol type="1">
<li><p><strong>Computational Efficiency</strong>: Despite being more efficient than alternatives, CLIP still requires enormous computational resources. The paper estimates that a 1000x increase in compute would be needed for zero-shot CLIP to match state-of-the-art supervised models on all tasks. Developing more compute-efficient training methods would make this approach more accessible.</p></li>
<li><p><strong>Few-Shot Performance</strong>: While CLIP excels at zero-shot tasks, the transition to few-shot learning is somewhat counterintuitive. Adding just a few examples sometimes decreases performance relative to zero-shot predictions. Developing methods that better integrate prior knowledge (from zero-shot) with example-based learning could yield significant improvements.</p></li>
<li><p><strong>Handling Abstract and Systematic Tasks</strong>: CLIP struggles with abstract and systematic tasks like counting objects in an image. This suggests limitations in how well natural language supervision captures certain visual reasoning capabilities. Combining CLIP’s approach with methods specifically designed for reasoning tasks might address this gap.</p></li>
<li><p><strong>Data Diversity and Bias</strong>: The web-scale training data inevitably contains biases present in internet text and images. While the paper acknowledges these issues, there’s significant room for improvement in developing methods to identify and mitigate these biases during training or inference.</p></li>
<li><p><strong>Integration with Video Understanding</strong>: While CLIP shows strong performance on action recognition from single frames, extending its capabilities to understand temporal dynamics in videos would be a valuable improvement. This might involve adapting the contrastive learning objective to include temporal information.</p></li>
<li><p><strong>Explainability</strong>: Like many deep learning models, CLIP’s decision-making process lacks transparency. Developing methods to explain CLIP’s predictions would increase trust and enable more effective human-AI collaboration. Recent work on feature visualization and attribution methods for multimodal models represents a promising direction for making CLIP’s decisions more interpretable.</p></li>
</ol>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<section id="primary-paper" class="level3">
<h3 class="anchored" data-anchor-id="primary-paper">Primary Paper</h3>
<ul>
<li>Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., &amp; Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. <em>arXiv preprint arXiv:2103.00020</em>.</li>
</ul>
</section>
<section id="resources-used-for-this-analysis" class="level3">
<h3 class="anchored" data-anchor-id="resources-used-for-this-analysis">Resources Used for This Analysis</h3>
<ul>
<li>OpenAI’s CLIP GitHub Repository: https://github.com/openai/CLIP</li>
<li>CLIP Paper Implementation in PyTorch: https://github.com/openai/CLIP/tree/main/clip</li>
<li>Hugging Face’s CLIP Documentation: https://huggingface.co/docs/transformers/model_doc/clip</li>
<li>Berkeley AI Research Blog (for reference on blog style): https://bair.berkeley.edu/blog/</li>
<li>Other Blogs and Youtube Videos on CLIP Model for better understanding</li>
<li>Github Repos of Custom CLIP Model codes and codes on experiments on CLIP Model</li>
<li>ChatGPT for grammar correction, language refinement, and writing assistance (helping to improve clarity and structure of my own analysis)</li>
</ul>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../about.html" class="pagination-link" aria-label="About">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">About</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->




</body></html>