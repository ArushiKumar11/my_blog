[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/arushi-mmdp2.html",
    "href": "posts/arushi-mmdp2.html",
    "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP): A Deep Dive",
    "section": "",
    "text": "— title: “Arushi’s MMdP2 Notebook” description: “CLIP exploration for MMdP2 project” date: 2025-05-08 categories: [multimodal, CLIP, project] format: ipynb —",
    "crumbs": [
      "About",
      "Posts",
      "Learning Transferable Visual Models From Natural Language Supervision (CLIP): A Deep Dive"
    ]
  },
  {
    "objectID": "posts/arushi-mmdp2.html#motivation",
    "href": "posts/arushi-mmdp2.html#motivation",
    "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP): A Deep Dive",
    "section": "Motivation",
    "text": "Motivation\nWhen OpenAI released CLIP (Contrastive Language-Image Pre-training) in 2021, it represented a fundamental shift in how we approach computer vision problems. Rather than training models on curated labeled datasets like ImageNet, CLIP demonstrated the power of learning from natural language supervision at scale.\nI chose to analyze this paper because of its impact on multimodal learning and its approach to solving a core challenge in machine learning: the transferability of models across diverse tasks without task-specific fine-tuning. It laid the foundation for developing many other multimodal systems.\nAlso CLIP’s zero-shot capabilities: The ability to perform well on previously unseen tasks without any additional training data. This approach not only scales more efficiently but also makes AI systems more adaptable and aligned with human intentions through natural language.\nAdditionally, CLIP’s architecture serves as a foundational element for many subsequent advancements in multimodal AI, including DALL-E, Stable Diffusion,CLAP, Meta’s ImageBind(which learns a joint embedding across six modalities – images, text, audio, depth, thermal, and IMU data) and other models. Understanding CLIP is crucial for comprehending the current landscape of multimodal learning and generative AI.",
    "crumbs": [
      "About",
      "Posts",
      "Learning Transferable Visual Models From Natural Language Supervision (CLIP): A Deep Dive"
    ]
  },
  {
    "objectID": "posts/arushi-mmdp2.html#historical-context-and-connection-to-multimodal-learning",
    "href": "posts/arushi-mmdp2.html#historical-context-and-connection-to-multimodal-learning",
    "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP): A Deep Dive",
    "section": "Historical Context and Connection to Multimodal Learning",
    "text": "Historical Context and Connection to Multimodal Learning\nScalable pre-training methods that learn directly from web-scale text collections have revolutionized NLP, enabling models like GPT-3 to perform zero-shot transfer across diverse tasks. CLIP extends this paradigm to computer vision, challenging the standard practice of relying on crowd-labeled datasets like ImageNet by learning visual representations directly from natural language supervision on web-scale data\n\nThe Evolution of Visual Representation Learning\n\nTraditional Supervised Learning (pre-2015): Models like AlexNet, VGG, and ResNet were trained on manually labeled datasets like ImageNet, requiring extensive human annotation and limiting models to fixed categories.\nSelf-Supervised Learning (2015-2020): Approaches like contrastive learning (SimCLR), masked image modeling, and rotation prediction attempted to learn representations without explicit labels, but still required task-specific fine-tuning for downstream tasks.\nWeakly-Supervised Learning: Methods like those used in BiT (Big Transfer) leveraged larger but noisier labeled datasets (JFT-300M) to improve transfer learning capabilities.\n\n\n\nEarly Explorations of Language-Vision Connections\nThe idea of using language to supervise visual learning wasn’t entirely new when CLIP emerged:\n\nIn 1999, Mori et al. explored improving image retrieval by training models to predict nouns and adjectives in paired text documents.\nQuattoni et al. (2007) demonstrated learning more data-efficient image representations by using text in captions.\nJoulin et al. (2016) showed that CNNs trained to predict words in image captions could learn useful representations.\nVisual N-Grams (Li et al., 2017) used text supervision and demonstrated limited zero-shot capabilities.\n\nHowever, these earlier approaches achieved limited success and couldn’t match the performance of supervised learning on standard benchmarks. For instance, Li et al. (2017) reached only 11.5% accuracy on ImageNet in a zero-shot setting.\nThe emergence of transformer-based models in NLP (BERT, GPT) showed the power of pre-training on large-scale text corpora. This inspired approaches to adapt similar techniques to computer vision and multimodal tasks:\n\nViLBERT, LXMERT, and other vision-language models (2019-2020) combined transformers for both modalities but required fine-tuning for downstream tasks.\nVirTex, ICMLM, and ConVIRT (2020) explored transformer-based language modeling and contrastive objectives for learning image representations.\n\n\n\nCLIP’s Innovations\nCLIP built upon these foundations but made several crucial innovations:\n\nScale: Training on 400 million image-text pairs, dwarfing previous approaches.\nSimplicity: Using a straightforward contrastive learning objective rather than complex architectures or objectives.\nZero-Shot Design: Explicitly optimizing for zero-shot transfer rather than treating it as a secondary capability.\nNatural Language Interface: Creating a flexible way to specify visual concepts through language prompts.",
    "crumbs": [
      "About",
      "Posts",
      "Learning Transferable Visual Models From Natural Language Supervision (CLIP): A Deep Dive"
    ]
  },
  {
    "objectID": "posts/arushi-mmdp2.html#clip-model-approach-and-training",
    "href": "posts/arushi-mmdp2.html#clip-model-approach-and-training",
    "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP): A Deep Dive",
    "section": "CLIP Model: Approach and Training?",
    "text": "CLIP Model: Approach and Training?\n\nWhat is CLIP?\nCLIP is a joint image and text embedding model trained using 400 million image and text pairs in a self supervised way. This means that it maps both text and images to the same embedding space. So, for example, an image of a dog and the sentence “an image of a dog” would end up having very similar embeddings and be close to each other in the vector space. Thus it learns visual concepts from natural language supervision. Unlike traditional vision models that are trained on fixed label sets, CLIP learns to connect images and text in a shared embedding space, enabling flexible zero-shot prediction for any visual concept that can be expressed in language. The model consists of two encoders:\n\nAn image encoder that maps images to a feature space\nA text encoder that maps text descriptions to the same feature space\n\nBy mapping both modalities to the same space, CLIP can measure the similarity between any image and any text, allowing it to perform a wide range of tasks without task-specific training data. ### Natural Language Supervision and Dataset Used CLIP learns from image captions and text descriptions rather than specific labels like “dog” or “cat.” This lets it learn from millions of images on the internet without needing people to manually label each one. The researchers created a new dataset called WebImageText (WIT) with 400 million image-text pairs from the internet. To ensure variety, they searched for images with text related to 500,000 different topics and included up to 20,000 examples for each topic.\n() Figure: CLIP is much more efficient at zero-shot transfer than image caption baselines. Transformer-based language models learn 3x slower than a baseline which predicts a bag-of-words encoding of text. Swapping to the contrastive objective used in CLIP further improves efficiency by 4x.\n\n\nContrastive Learning\nContrastive learning is a technique used in machine learning, particularly in the field of unsupervised learning. Contrastive learning is a method where we teach an AI model to recognize similarities and differences of a large number of data points.\nWe have a main item (the “anchor sample”), a similar item (“positive”), and a different item (“negative sample”). The goal is to make the model understand that the anchor and the positive item are alike, so it brings them closer together in its mind while recognizing that the negative item is different and pushing it away. A similar or “positive” image or might be from the same category (e.g., dogs) as the main image or a modified version of it, whereas a “negative” image would be entirely different, typically from another category (e.g., cats).\n\n\n\nimage.png\n\n\nCLIP employs contrastive learning to align images with their corresponding text descriptions. The key aspects of this approach include:\n\nTraining objective: Given a batch of N (image, text) pairs, CLIP learns to identify which of the N² possible (image, text) pairings actually occurred.\nSimilarity maximization: For real pairs, CLIP maximizes the cosine similarity between image and text embeddings.\nNegative sample contrast: Simultaneously, CLIP minimizes similarity between non-matching pairs (all other N²-N combinations).\nSymmetric loss function: CLIP uses a symmetric cross-entropy loss that treats both image-to-text and text-to-image prediction equally.\n\nPseudocode is as follows\n\n'''\n# image_encoder - ResNet or Vision Transformer\n# text_encoder - CBOW or Text Transformer\n# I[n, h, w, c] - minibatch of aligned images\n# T[n, l] - minibatch of aligned texts\n# W_i[d_i, d_e] - learned proj of image to embed\n# W_t[d_t, d_e] - learned proj of text to embed\n# t - learned temperature parameter\n# extract feature representations of each modality\nI_f = image_encoder(I) #[n, d_i]\nT_f = text_encoder(T) #[n, d_t]\n# joint multimodal embedding [n, d_e]\nI_e = l2_normalize(np.dot(I_f, W_i), axis=1)\nT_e = l2_normalize(np.dot(T_f, W_t), axis=1)\n# scaled pairwise cosine similarities [n, n]\nlogits = np.dot(I_e, T_e.T) * np.exp(t)\n# symmetric loss function\nlabels = np.arange(n)\nloss_i = cross_entropy_loss(logits, labels, axis=0)\nloss_t = cross_entropy_loss(logits, labels, axis=1)\nloss = (loss_i + loss_t)/2\n'''\n\n'\\n# image_encoder - ResNet or Vision Transformer\\n# text_encoder - CBOW or Text Transformer\\n# I[n, h, w, c] - minibatch of aligned images\\n# T[n, l] - minibatch of aligned texts\\n# W_i[d_i, d_e] - learned proj of image to embed\\n# W_t[d_t, d_e] - learned proj of text to embed\\n# t - learned temperature parameter\\n# extract feature representations of each modality\\nI_f = image_encoder(I) #[n, d_i]\\nT_f = text_encoder(T) #[n, d_t]\\n# joint multimodal embedding [n, d_e]\\nI_e = l2_normalize(np.dot(I_f, W_i), axis=1)\\nT_e = l2_normalize(np.dot(T_f, W_t), axis=1)\\n# scaled pairwise cosine similarities [n, n]\\nlogits = np.dot(I_e, T_e.T) * np.exp(t)\\n# symmetric loss function\\nlabels = np.arange(n)\\nloss_i = cross_entropy_loss(logits, labels, axis=0)\\nloss_t = cross_entropy_loss(logits, labels, axis=1)\\nloss = (loss_i + loss_t)/2\\n'\n\n\n\n\nArchitecture of CLIP\nClIP uses two separate architectures as the backbone for encoding vision and text datasets:\n\nimage_encoder: Represents the neural network architecture (e.g., ResNet or Vision Transformer) responsible for encoding images.\ntext_encoder: Represents the neural network architecture (e.g., CBOW, BERT, or Text Transformer) responsible for encoding textual information.\nShared Embedding Space: The two encoders produce embeddings in a shared vector space. These shared embedding spaces allow CLIP to compare text and image representations and learn their underlying relationships.\n\nThe original CLIP model was trained from scratch without initializing the image encoder and the text encoder with pre-trained weights due to the large volume of the dataset (400 million image-text pairs) that they used to train their CLIP model.\n\n\nTraining Architecture\n\nfirst step : Contrastive Pre-training\n CLIP is pre-trained on a large-scale dataset of 400 million (image, text data) pairs collected from the internet. During pre-training, the model is presented with pairs of images and text captions.Thus shared latent space embeddings are created.\n\n\nsecond step :Create Dataset Classifiers from Label Text\nFor each image, multiple text descriptions are created, including the correct one and several incorrect ones. This creates a mix of positive samples (matching) and negative sample (mismatched) pairs. These descriptions are fed into the text encoder, generating class-specific embeddings. And then, Contrastive Loss Function is used\n\n\nthird step: Zero shot Classification\nNow, the trained text encoder is used as a zero-shot classifier. With a new image, CLIP can make zero-shot predictions. This is done by passing it through the image encoder and the dataset classifier without fine-tuning.",
    "crumbs": [
      "About",
      "Posts",
      "Learning Transferable Visual Models From Natural Language Supervision (CLIP): A Deep Dive"
    ]
  },
  {
    "objectID": "posts/arushi-mmdp2.html#clip-applications",
    "href": "posts/arushi-mmdp2.html#clip-applications",
    "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP): A Deep Dive",
    "section": "CLIP Applications",
    "text": "CLIP Applications\n\nIntegration of NLP and image processing tasks:\nexample tasks include - Generating text descriptions for images,Classify images based on textual descriptions, Edit images based on textual prompts. \n\n\nContent Moderation\nCLIP can be used to moderate content on online platforms by analyzing images and accompanying text to identify and filter out inappropriate or harmful content.\n\n\nBasis for other Models :\nConcept of CLIP, along with its techniques, extends beyond images and text to embrace other modalities. Netflix, in thisblog post, trained a model by combining video and text modalities in the common embedding space to enhance search within video applications. Contrastive Language-Audio Pretraining (CLAP) is another model that integrates text and audio modalities within the same embedding space, making it valuable for improving search functionalities within audio applications.",
    "crumbs": [
      "About",
      "Posts",
      "Learning Transferable Visual Models From Natural Language Supervision (CLIP): A Deep Dive"
    ]
  },
  {
    "objectID": "posts/arushi-mmdp2.html#key-learnings-from-clip",
    "href": "posts/arushi-mmdp2.html#key-learnings-from-clip",
    "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP): A Deep Dive",
    "section": "Key Learnings from CLIP",
    "text": "Key Learnings from CLIP\n\n1. The Power of Natural Language Supervision\nCLIP demonstrates that natural language can serve as a rich, flexible form of supervision for visual models. The advantages include:\n\nScalability: Leveraging existing image-text pairs from the internet without requiring manual labels.\nExpressivity: Capturing nuanced visual concepts beyond simple object categories.\nTask-Agnosticity: Supporting a wide range of downstream tasks without specialized architectures.\n\n\n\n2. Contrastive Learning as an Efficient Pre-training Strategy\nThe paper provides evidence that contrastive learning between image and text embeddings is more computationally efficient than alternative approaches:\n\n4x more efficient than bag-of-words prediction.\n12x more efficient than transformer-based language modeling.\n\nThis efficiency allowed CLIP to scale to hundreds of millions of training examples with available compute resources.\n\n\n3. Zero-Shot Transfer as a Primary Capability\nRather than treating zero-shot transfer as a secondary capability, CLIP is explicitly designed to excel at it:\n\nThe contrastive pre-training can be viewed as optimizing performance on a proxy computer vision dataset with 32,768 randomly created classes defined via natural language.\nZero-shot classifiers are created by embedding class names or descriptions in the same space as images.\n\nThis approach as per the paper achieves impressive results across diverse tasks including OCR, action recognition, geo-localization, and many types of fine-grained classification.\n\n\n4. The Role of Prompt Engineering\nThe paper introduces the concept of “prompt engineering” for vision models - crafting text templates that help specify the context for classification:\n\nUsing templates like “A photo of a {label}” instead of just the label text improves performance.\nContext-specific prompts (e.g., “A photo of a {label}, a type of pet” for pet classification) further boost accuracy.\nEnsembling across multiple prompts provides additional gains (5% improvement on ImageNet).\n\nThis connection to similar techniques in language models like GPT-3 highlights the convergence of language and vision paradigms.\n\n\n5. Robustness to Distribution Shift\nOne of the most striking findings is that zero-shot CLIP models are significantly more robust to distribution shifts than traditional supervised models:\n\nThe gap between performance on ImageNet and on various distribution-shifted datasets (ImageNetV2, ImageNet Sketch, etc.) is reduced by up to 75%.\nThis suggests that task-agnostic pre-training may inherently lead to more robust visual representations.\n\n\n\nAdvantages of CLIP over Traditional Vision Models\n\nDeeper Understanding of Images\nWhile traditional vision models can identify objects in images, CLIP goes further by understanding relationships and context. It can not only recognize a child in a park but also infer activities and emotions like “playing” or “having fun.”\n\n\nImproved Data Efficiency\nTraditional vision models require massive labeled datasets that are expensive and time-consuming to create. CLIP learns from natural language descriptions paired with images, reducing the need for manual labeling and making it more efficient for specialized domains with limited data.\n\n\nBetter Generalization and Interpretability\nCLIP’s training on diverse image-text pairs helps it generalize to new scenarios without specific training. Its connection to language also improves explainability - instead of just classifying an image, it can express its understanding through text, making its reasoning more transparent to users.",
    "crumbs": [
      "About",
      "Posts",
      "Learning Transferable Visual Models From Natural Language Supervision (CLIP): A Deep Dive"
    ]
  },
  {
    "objectID": "posts/arushi-mmdp2.html#exploring-clip-defining-a-custom-clip-model",
    "href": "posts/arushi-mmdp2.html#exploring-clip-defining-a-custom-clip-model",
    "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP): A Deep Dive",
    "section": "Exploring CLIP: Defining a Custom CLIP Model",
    "text": "Exploring CLIP: Defining a Custom CLIP Model\nWe define a custom clip model and train it on flickr30 dataset\nGiven the compute and training resource required we use pre trained resnet\n\n!pip install datasets\n\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow&gt;=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill&lt;0.3.9,&gt;=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests&gt;=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm&gt;=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess&lt;0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec&lt;=2024.12.0,&gt;=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]&lt;=2024.12.0,&gt;=2023.1.0-&gt;datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: huggingface-hub&gt;=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (2.6.1)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (1.3.2)\nRequirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (25.3.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (1.5.0)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (6.2.0)\nRequirement already satisfied: propcache&gt;=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (0.3.1)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (1.19.0)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub&gt;=0.24.0-&gt;datasets) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy&gt;=1.17-&gt;datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy&gt;=1.17-&gt;datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy&gt;=1.17-&gt;datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy&gt;=1.17-&gt;datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy&gt;=1.17-&gt;datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy&gt;=1.17-&gt;datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.32.2-&gt;datasets) (3.4.1)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.32.2-&gt;datasets) (3.10)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.32.2-&gt;datasets) (2.3.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.32.2-&gt;datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas-&gt;datasets) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas-&gt;datasets) (2025.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas-&gt;datasets) (2025.2)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;datasets) (1.17.0)\nRequirement already satisfied: intel-openmp&lt;2026,&gt;=2024 in /usr/local/lib/python3.11/dist-packages (from mkl-&gt;numpy&gt;=1.17-&gt;datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl-&gt;numpy&gt;=1.17-&gt;datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*-&gt;mkl-&gt;numpy&gt;=1.17-&gt;datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath-&gt;numpy&gt;=1.17-&gt;datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp&lt;2026,&gt;=2024-&gt;mkl-&gt;numpy&gt;=1.17-&gt;datasets) (2024.2.0)\n\n\n\ntrain on flickr30 dataset\nhere we load dataset and create a dataloader\n\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom torchvision import transforms\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms\nfrom PIL import Image\n# Define a custom dataset class for Flickr30k\nclass Flickr30kDataset(torch.utils.data.Dataset):\n    def __init__(self):\n        self.dataset = load_dataset(\"nlphuji/flickr30k\", cache_dir=\"./huggingface_data\")\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n        ])\n        self.cap_per_image = 2\n\n    def __len__(self):\n        return self.dataset.num_rows[\"test\"] * self.cap_per_image\n\n    def __getitem__(self, idx):\n        original_idx = idx // self.cap_per_image\n        image = self.dataset[\"test\"][original_idx][\"image\"].convert(\"RGB\")\n        image = self.transform(image)\n        caption = self.dataset[\"test\"][original_idx][\"caption\"][idx % self.cap_per_image]\n\n        return {\"image\": image, \"caption\": caption}\n\n# Create an instance of the custom dataset\nflickr30k_custom_dataset = Flickr30kDataset()\n\n\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass Config:\n    \"\"\"\n    Configuration class for the CLIP training script.\n    \"\"\"\n\n    embed_dim: int = 512  # Embedding dimension\n    transformer_embed_dim: int = 768  # Transformer embedding dimension\n    max_len: int = 32  # Maximum text length\n    text_model: str = \"distilbert-base-multilingual-cased\"  # Text model name\n    epochs: int = 5  # Number of training epochs\n    batch_size: int = 128  # Batch size\n\n\n# Create the DataLoader\nclip_dataloader = DataLoader(flickr30k_custom_dataset, batch_size=Config.batch_size, shuffle=True, num_workers=4)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Create an iterator from the dataloader\ndata_iter = iter(clip_dataloader)\n\n# Get one batch\nbatch = next(data_iter)\n\n\nimage = batch[\"image\"][0]  # Assuming batch size is greater than 0\ncaption = batch[\"caption\"][0]\n\n# Convert the image tensor to a NumPy array and permute dimensions\nimage_np = np.transpose(image.numpy(), (1, 2, 0))\n\n# Display the image and caption\nplt.imshow(image_np)\nplt.title(f\"Caption: {caption}\")\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(\"example of a caption in abatch:\",  batch[\"caption\"][0])\nprint(\"number of captions in each batch:\",len(batch[\"caption\"]))\n\nexample of a caption in abatch: A man, trying to hold on, as he competes in a rodeo on horse back.\nnumber of captions in each batch: 128\n\n\n\n\ndefining CLIP loss\n\n!pip install git+https://github.com/openai/CLIP.git --quiet\n\n  Preparing metadata (setup.py) ... done\n\n\n\nimport torch.nn.functional as F\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ndef CLIP_loss(logits: torch.Tensor) -&gt; torch.Tensor:\n    # Assuming n is the number of classes\n    n = logits.shape[1]\n\n    # Create labels tensor\n    labels = torch.arange(n).to(device)\n\n    # Calculate cross entropy losses along axis 0 and 1\n    loss_i = F.cross_entropy(logits.transpose(0, 1), labels, reduction=\"mean\")\n    loss_t = F.cross_entropy(logits, labels, reduction=\"mean\")\n\n    # Calculate the final loss\n    loss = (loss_i + loss_t) / 2\n\n    return loss\n\ndef metrics(similarity: torch.Tensor):\n    y = torch.arange(len(similarity)).to(similarity.device)\n    img2cap_match_idx = similarity.argmax(dim=1)\n    cap2img_match_idx = similarity.argmax(dim=0)\n\n    img_acc = (img2cap_match_idx == y).float().mean()\n    cap_acc = (cap2img_match_idx == y).float().mean()\n\n    return img_acc, cap_acc\n\n\nimport clip\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n\nimage = batch[\"image\"].to(device)\ntrue_text = batch[\"caption\"]\nwrong_text = true_text[::-1]\n\n\nfor captions in [true_text, wrong_text]:\n    text = clip.tokenize(captions).to(device)\n\n    # with torch.no_grad():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n\n    # normalized features\n    image_features = image_features / image_features.norm(dim=1, keepdim=True)\n    text_features = text_features / text_features.norm(dim=1, keepdim=True)\n    similarity = text_features @ image_features.T\n    loss = CLIP_loss(similarity)\n    print(loss)\n\ntensor(4.7188, device='cuda:0', dtype=torch.float16, grad_fn=&lt;DivBackward0&gt;)\ntensor(4.8555, device='cuda:0', dtype=torch.float16, grad_fn=&lt;DivBackward0&gt;)\n\n\n\n\ndefining projection and encoder layers\n\nfrom transformers import AutoModel, AutoTokenizer, BertTokenizer\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\n\nclass Projection(nn.Module):\n    def __init__(self, d_in: int, d_out: int, p: float = 0.5) -&gt; None:\n        super().__init__()\n        self.linear1 = nn.Linear(d_in, d_out, bias=False)\n        self.linear2 = nn.Linear(d_out, d_out, bias=False)\n        self.layer_norm = nn.LayerNorm(d_out)\n        self.drop = nn.Dropout(p)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        embed1 = self.linear1(x)\n        embed2 = self.drop(self.linear2(F.gelu(embed1)))\n        embeds = self.layer_norm(embed1 + embed2)\n        return embeds\n\n\nclass VisionEncoder(nn.Module):\n    def __init__(self, d_out: int) -&gt; None:\n        super().__init__()\n        base = models.resnet34(pretrained=True)\n        d_in = base.fc.in_features\n        base.fc = nn.Identity()\n        self.base = base\n        self.projection = Projection(d_in, d_out)\n        for p in self.base.parameters():\n            p.requires_grad = False\n\n    def forward(self, x):\n        projected_vec = self.projection(self.base(x))\n        projection_len = torch.norm(projected_vec, dim=-1, keepdim=True)\n        return projected_vec / projection_len\n\n\nclass TextEncoder(nn.Module):\n    def __init__(self, d_out: int) -&gt; None:\n        super().__init__()\n        self.base = AutoModel.from_pretrained(Config.text_model)\n        self.projection = Projection(Config.transformer_embed_dim, d_out)\n        for p in self.base.parameters():\n            p.requires_grad = False\n\n    def forward(self, x):\n        out = self.base(x)[0]\n        out = out[:, 0, :]  # get CLS token output\n        projected_vec = self.projection(out)\n        projection_len = torch.norm(projected_vec, dim=-1, keepdim=True)\n        return projected_vec / projection_len\n\n\nclass Tokenizer:\n    def __init__(self, tokenizer: BertTokenizer) -&gt; None:\n        self.tokenizer = tokenizer\n\n    def __call__(self, x: str) -&gt; AutoTokenizer:\n        return self.tokenizer(\n            x, max_length=Config.max_len, truncation=True, padding=True, return_tensors=\"pt\"\n        )\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nimage = batch[\"image\"].to(device)\ntrue_text = batch[\"caption\"]\nwrong_text = true_text[::-1]\n\nvision_encoder = VisionEncoder(Config.embed_dim).to(device)\ncaption_encoder = TextEncoder(Config.embed_dim).to(device)\ntokenizer = Tokenizer(AutoTokenizer.from_pretrained(Config.text_model))\n\nfor captions in [true_text, wrong_text]:\n    text = tokenizer(captions).to(device)\n\n    # with torch.no_grad():\n    image_features = vision_encoder(image)\n    text_features = caption_encoder(text[\"input_ids\"])\n\n    # normalized features\n    image_features = image_features / image_features.norm(dim=1, keepdim=True)\n    text_features = text_features / text_features.norm(dim=1, keepdim=True)\n    similarity = text_features @ image_features.T\n    loss = CLIP_loss(similarity)\n    print(loss)\n\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|██████████| 83.3M/83.3M [00:01&lt;00:00, 85.9MB/s]\n\n\n\n\n\n2025-05-08 13:42:47.955754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746711768.127893     193 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746711768.178668     193 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n\n\ntensor(4.8544, device='cuda:0', grad_fn=&lt;DivBackward0&gt;)\ntensor(4.8516, device='cuda:0', grad_fn=&lt;DivBackward0&gt;)\n\n\n\n\ndefine class for custom clip model\n\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer\nfrom typing import List, Tuple\n\n\nclass CustomModel(nn.Module):\n    def __init__(self, lr: float = 1e-3) -&gt; None:\n        super().__init__()\n        self.vision_encoder = VisionEncoder(Config.embed_dim)\n        self.caption_encoder = TextEncoder(Config.embed_dim)\n        self.tokenizer = Tokenizer(AutoTokenizer.from_pretrained(Config.text_model, use_fast=False))\n        self.lr = lr\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    def forward(self, images, text):\n        text = self.tokenizer(text).to(self.device)\n\n        image_embed = self.vision_encoder(images)\n        caption_embed = self.caption_encoder(text[\"input_ids\"])\n        similarity = caption_embed @ image_embed.T\n\n        loss = CLIP_loss(similarity)\n        img_acc, cap_acc = metrics(similarity)\n        return loss, img_acc, cap_acc\n\n\n\n# Create an instance of your model\nmodel = CustomModel().to(device)\n\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\n# Define optimizer\noptimizer = torch.optim.Adam([\n    {'params': model.vision_encoder.parameters()},\n    {'params': model.caption_encoder.parameters()}\n], lr=model.lr)\n\n\n\ntrain custom model for three epochs\n\nstart_epoch = 0\nnum_epochs = 3\n\nbatch_zero = True\nfor epoch in range(start_epoch, num_epochs):\n    model.train()\n    for batch in clip_dataloader:\n        image = batch[\"image\"].to(device)\n        text = batch[\"caption\"]\n        # images, text = batch\n        loss, img_acc, cap_acc = model(image, text)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch_zero:\n          print(f\"Epoch [{0}/{num_epochs}], Batch Loss: {loss.item()}\")\n          batch_zero = False\n\n\n    # Print training statistics\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Batch Loss: {loss.item()}\")\n\nprint(\"Training complete.\")\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\nEpoch [0/3], Batch Loss: 4.852700233459473\nEpoch [1/3], Batch Loss: 3.8830089569091797\n\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\nEpoch [2/3], Batch Loss: 3.8700060844421387\n\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\nEpoch [3/3], Batch Loss: 3.8735854625701904\nTraining complete.\n\n\n\n\nZero-Shot Image Classification using CLIP\nLet’s demonstrate CLIP’s zero-shot classification capabilities with a simple example:\n\n# Install required libraries\n!pip install torch torchvision ftfy regex tqdm pillow matplotlib\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\nRequirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (6.3.1)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions&gt;=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1-&gt;torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy-&gt;torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy-&gt;torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy-&gt;torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy-&gt;torchvision) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy-&gt;torchvision) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy-&gt;torchvision) (2.4.1)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.17.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2-&gt;torch) (3.0.2)\nRequirement already satisfied: intel-openmp&lt;2026,&gt;=2024 in /usr/local/lib/python3.11/dist-packages (from mkl-&gt;numpy-&gt;torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl-&gt;numpy-&gt;torchvision) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*-&gt;mkl-&gt;numpy-&gt;torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath-&gt;numpy-&gt;torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp&lt;2026,&gt;=2024-&gt;mkl-&gt;numpy-&gt;torchvision) (2024.2.0)\n\n\n\n# Import libraries\nimport torch\nimport clip\nfrom PIL import Image\nimport requests\nimport matplotlib.pyplot as plt\nfrom torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\nfrom torchvision.transforms.functional import InterpolationMode\nimport numpy as np\n\n# Load the model\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n\n\n# Download an example image\nurl=\"https://images.pexels.com/photos/1001976/pexels-photo-1001976.jpeg?cs=srgb&dl=pexels-hemant-gupta-374105-1001976.jpg&fm=jpg\"\n#url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Golden_Retriever_Hund_Dog.JPG/1280px-Golden_Retriever_Hund_Dog.JPG\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n\n# Prepare the image for CLIP\nimage_input = preprocess(image).unsqueeze(0).to(device)\n\ndog_breeds = [\n    \"golden retriever\", \"labrador retriever\", \"poodle\", \"german shepherd\", \n    \"bulldog\", \"beagle\", \"rottweiler\", \"siberian husky\", \"dachshund\"\n]\n\n# Apply prompt engineering\ntext_inputs = torch.cat([clip.tokenize(f\"a photo of a {breed}, a type of dog\") for breed in dog_breeds]).to(device)\n\n# Calculate features\nwith torch.no_grad():\n    image_features = model.encode_image(image_input)\n    text_features = model.encode_text(text_inputs)\n    \n    # Normalize features\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n    text_features /= text_features.norm(dim=-1, keepdim=True)\n    \n    # Calculate similarity\n    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\n# Print results\nfor i, breed in enumerate(dog_breeds):\n    print(f\"{breed}: {similarity[0][i].item():.2%}\")\n\n# Visualize results\nplt.figure(figsize=(10, 5))\nplt.bar(dog_breeds, similarity[0].cpu().numpy() * 100)\nplt.xlabel('Dog Breeds')\nplt.ylabel('Confidence (%)')\nplt.title('CLIP Zero-Shot Classification')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\ngolden retriever: 98.88%\nlabrador retriever: 1.03%\npoodle: 0.03%\ngerman shepherd: 0.00%\nbulldog: 0.01%\nbeagle: 0.01%\nrottweiler: 0.00%\nsiberian husky: 0.00%\ndachshund: 0.03%\n\n\n\n\n\n\n\n\n\n\nconclusion:\nit performs zero shot classification correctly\n\n\n\nVisualizing CLIP’s Embedding Space\nLet’s explore CLIP’s embedding space by visualizing the text embeddings for various categories:\n\nfrom sklearn.manifold import TSNE\n\n# Define categories across different domains\ncategories = {\n    'Animals': ['dog', 'cat', 'bird', 'fish', 'tiger', 'elephant', 'snake', 'bear'],\n    'Vehicles': ['car', 'motorcycle', 'bicycle', 'bus', 'train', 'airplane', 'boat', 'helicopter'],\n    'Food': ['pizza', 'hamburger', 'sushi', 'salad', 'cake', 'ice cream', 'apple', 'banana'],\n    'Clothing': ['shirt', 'pants', 'dress', 'hat', 'shoes', 'jacket', 'sweater', 'socks']\n}\n\n# Flatten the categories\nall_categories = [item for sublist in categories.values() for item in sublist]\ndomain_labels = [domain for domain, items in categories.items() for _ in items]\n\n# Apply prompt engineering\nprompts = [f\"a photo of a {category}\" for category in all_categories]\ntext_tokens = clip.tokenize(prompts).to(device)\n\n# Get text embeddings\nwith torch.no_grad():\n    text_features = model.encode_text(text_tokens)\n    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n\n# Reduce to 2D using t-SNE\ntsne = TSNE(n_components=2, random_state=42)\ntext_features_2d = tsne.fit_transform(text_features.cpu().numpy())\n\n# Create a colormap for domains\ndomain_to_color = {\n    'Animals': 'red',\n    'Vehicles': 'blue',\n    'Food': 'green',\n    'Clothing': 'purple'\n}\ncolors = [domain_to_color[domain] for domain in domain_labels]\n\n# Plot\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(text_features_2d[:, 0], text_features_2d[:, 1], c=colors, alpha=0.7)\n\n# Add labels\nfor i, category in enumerate(all_categories):\n    plt.annotate(category, (text_features_2d[i, 0], text_features_2d[i, 1]))\n\n# Add legend\nlegend_elements = [plt.Line2D([0], [0], marker='o', color='w', \n                             markerfacecolor=color, markersize=10, label=domain)\n                  for domain, color in domain_to_color.items()]\nplt.legend(handles=legend_elements)\n\nplt.title('t-SNE Visualization of CLIP Text Embeddings')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nconclusion:\nnote how in the embedding space, items of similar kind are together. That is all vehicles are together, all food items are together and so on\n\n\n\nExploring Prompt Engineering\nLet’s examine the impact of different prompt templates on CLIP’s zero-shot performance:\n\n\nurl = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Giant_Panda_2004-03-2.jpg/1200px-Giant_Panda_2004-03-2.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nplt.figure(figsize=(6, 6))\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n\nimage_input = preprocess(image).unsqueeze(0).to(device)\n\nanimal_classes = [\"panda\", \"bear\", \"cat\", \"dog\", \"tiger\"]\n\nprompt_templates = [\n    \"{}.\",  # Just the label\n    \"a photo of a {}.\",  # Basic template\n    \"a photo of a {}, a type of animal.\",  # With category context\n    \"a close-up photo of a {}.\",  # With visual context\n    \"a {} in the wild.\"  # With environment context\n]\n\nresults = np.zeros((len(prompt_templates), len(animal_classes)))\n\nfor i, template in enumerate(prompt_templates):\n    prompts = [template.format(animal_class) for animal_class in animal_classes]\n    text_tokens = clip.tokenize(prompts).to(device)\n    \n    with torch.no_grad():\n        # Encode the image and text\n        image_features = model.encode_image(image_input)\n        text_features = model.encode_text(text_tokens)\n        \n        # Normalize features\n        image_features /= image_features.norm(dim=-1, keepdim=True)\n        text_features /= text_features.norm(dim=-1, keepdim=True)\n        \n        # Calculate similarity scores\n        similarity =  100*(image_features @ text_features.T).softmax(dim=-1)\n        results[i] = similarity[0].cpu().numpy()\n# Print similarity scores instead of bar chart\nfor i, template in enumerate(prompt_templates):\n    print(f\"\\nPrompt template: \\\"{template}\\\"\")\n    for cls, score in zip(animal_classes, results[i]):\n        print(f\"  {cls:&lt;10}: {score:.2f}%\")\n\n\n\n\n\n\n\n\n\n\nPrompt template: \"{}.\"\n  panda     : 21.48%\n  bear      : 20.67%\n  cat       : 18.78%\n  dog       : 19.44%\n  tiger     : 19.61%\n\nPrompt template: \"a photo of a {}.\"\n  panda     : 21.25%\n  bear      : 20.70%\n  cat       : 19.06%\n  dog       : 19.52%\n  tiger     : 19.47%\n\nPrompt template: \"a photo of a {}, a type of animal.\"\n  panda     : 21.34%\n  bear      : 20.66%\n  cat       : 19.11%\n  dog       : 19.47%\n  tiger     : 19.42%\n\nPrompt template: \"a close-up photo of a {}.\"\n  panda     : 21.17%\n  bear      : 20.75%\n  cat       : 19.11%\n  dog       : 19.56%\n  tiger     : 19.39%\n\nPrompt template: \"a {} in the wild.\"\n  panda     : 21.36%\n  bear      : 20.56%\n  cat       : 19.20%\n  dog       : 19.47%\n  tiger     : 19.41%\n\n\n\nconclusion :\nCLIP finds Panda and Bear to be closeby in embedding space. Also additional information like in the wild doesnt seem to help CLIP possibly because the image as we see doesnt have “wild” or forest in background\n\n\n\nExamining Robustness to Distribution Shift\nLet’s visualize CLIP’s robustness to distribution shift by comparing its performance on different image styles:\n\n# Define image URLs for different styles of the same object\nimage_urls = {\n    \"Photo\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Oranges_-_whole-halved-segment.jpg/1200px-Oranges_-_whole-halved-segment.jpg\",\n    \"Sketch\": \"https://i.ytimg.com/vi/5eOh2gFcXcQ/maxresdefault.jpg\",\n    \"Painting\": \"https://i.ytimg.com/vi/GJ0NlhSyG8A/maxresdefault.jpg\",\n    \"Cartoon\": \"https://www.shutterstock.com/image-vector/bright-vector-set-colorful-slice-260nw-604042622.jpg\"\n}\n\nfruit_classes = [\"orange\", \"apple\", \"banana\", \"strawberry\", \"pear\"]\nprompt_template = \"a photo of a {}, a type of fruit.\"\n\ntext_inputs = torch.cat([clip.tokenize(prompt_template.format(c)) for c in fruit_classes]).to(device)\nwith torch.no_grad():\n    text_features = model.encode_text(text_inputs)\n    text_features /= text_features.norm(dim=-1, keepdim=True)\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.flatten()\n\nresults = {}\nfor i, (style, url) in enumerate(image_urls.items()):\n    # Load and display image\n    image = Image.open(requests.get(url, stream=True).raw)\n    axes[i].imshow(image)\n    axes[i].set_title(f\"Style: {style}\")\n    axes[i].axis('off')\n\n    \n\n    image_input = preprocess(image).unsqueeze(0).to(device)\n    with torch.no_grad():\n        image_features = model.encode_image(image_input)\n        image_features /= image_features.norm(dim=-1, keepdim=True)\n        \n        # Calculate similarity\n        similarity = 100.0 * (image_features @ text_features.T).softmax(dim=-1)\n        results[style] = similarity[0].cpu().numpy()\n        \n        prediction = fruit_classes[similarity[0].argmax().item()]\n        confidence = similarity[0].max().item() \n        axes[i].text(10, 30, f\"Prediction: {prediction}\\nConfidence: {confidence:.1f}%\", \n                    bbox=dict(facecolor='white', alpha=0.7))\n# Print confidence scores for all fruit classes\nfor style, scores in results.items():\n    print(f\"\\nConfidence scores for style: {style}\")\n    for fruit, score in zip(fruit_classes, scores):\n        print(f\"  {fruit:&lt;10}: {score:.2f}%\")\n\nplt.tight_layout()\nplt.show()\n\n\n\nConfidence scores for style: Photo\n  orange    : 21.52%\n  apple     : 19.88%\n  banana    : 19.39%\n  strawberry: 19.64%\n  pear      : 19.58%\n\nConfidence scores for style: Sketch\n  orange    : 20.16%\n  apple     : 20.22%\n  banana    : 19.34%\n  strawberry: 19.86%\n  pear      : 20.44%\n\nConfidence scores for style: Painting\n  orange    : 21.09%\n  apple     : 20.08%\n  banana    : 19.44%\n  strawberry: 19.70%\n  pear      : 19.67%\n\nConfidence scores for style: Cartoon\n  orange    : 21.23%\n  apple     : 19.89%\n  banana    : 19.53%\n  strawberry: 19.62%\n  pear      : 19.72%\n\n\n\n\n\n\n\n\n\n\nconclusion:\nCLIP is able to work on images from different distribtuions ( like sketches/ photo/painting etc). For sketches we see it wrongly predicts pear instead\n\n\n\ncalculating distance between text embedding and image embedding\n\n\n\n# Load an image\nurl = \"https://cdn.shopify.com/s/files/1/0086/0795/7054/files/Golden-Retriever.jpg?v=1645179525\"\nresponse = requests.get(url)\nimage = Image.open(BytesIO(response.content))\n# Display the image\nplt.figure(figsize=(6, 6))\nplt.imshow(image)\nplt.axis('off')\nplt.show()\nimage_input = preprocess(image).unsqueeze(0).to(device)\n\n# Define classes to classify against\nclasses = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a car\", \"a photo of a house\"]\ntext_inputs = torch.cat([clip.tokenize(c) for c in classes]).to(device)\n\n# Calculate features\nwith torch.no_grad():\n    image_features = model.encode_image(image_input)\n    text_features = model.encode_text(text_inputs)\n\n# Normalize features\nimage_features /= image_features.norm(dim=-1, keepdim=True)\ntext_features /= text_features.norm(dim=-1, keepdim=True)\n\n# Calculate similarity\nsimilarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\nvalues, indices = similarity[0].topk(len(classes))\n\n# Print results\nfor value, index in zip(values, indices):\n    print(f\"{classes[index]:&gt;16s}: {100 * value.item():.2f}%\")\n\n\n\n\n\n\n\n\na photo of a dog: 99.85%\na photo of a cat: 0.15%\na photo of a house: 0.01%\na photo of a car: 0.00%\n\n\n\n\nusing CLIP to retrieve image from database guven a text prompt\n\nimport os\nfrom tqdm import tqdm\n# Function to encode images from a folder\ndef encode_image_folder(folder_path):\n    encoded_images = []\n    image_paths = []\n    \n    for filename in tqdm(os.listdir(folder_path)):\n        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n            try:\n                image_path = os.path.join(folder_path, filename)\n                image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n                \n                with torch.no_grad():\n                    image_features = model.encode_image(image)\n                    image_features /= image_features.norm(dim=-1, keepdim=True)\n                \n                encoded_images.append(image_features.cpu().numpy())\n                image_paths.append(image_path)\n            except Exception as e:\n                print(f\"Error processing {filename}: {e}\")\n    \n    return np.vstack(encoded_images), image_paths\n\n# Search images by text query\ndef search_images(text_query, encoded_images, image_paths, top_k=5):\n    with torch.no_grad():\n        text_input = clip.tokenize([text_query]).to(device)\n        text_features = model.encode_text(text_input)\n        text_features /= text_features.norm(dim=-1, keepdim=True)\n    \n    # Calculate similarities\n    similarities = (100.0 * torch.from_numpy(encoded_images) @ text_features.T.cpu()).numpy()\n    \n    # Get top matches\n    best_image_idx = np.argsort(similarities.flatten())[::-1][:top_k]\n    \n    return [(image_paths[idx], similarities[idx][0]) for idx in best_image_idx]\n\n# Demo the image search\nencoded_images, image_paths = encode_image_folder(\"/kaggle/input/image-folder/folder_with_images\")\n\nqueries = [\"a sunset over mountains\", \"a dog playing in the park\", \"food on a plate\"]\n\nplt.figure(figsize=(15, 5 * len(queries)))\n\nfor i, query in enumerate(queries):\n    results = search_images(query, encoded_images, image_paths)\n    \n    for j, (image_path, score) in enumerate(results):\n        plt.subplot(len(queries), 5, i * 5 + j + 1)\n        plt.imshow(Image.open(image_path))\n        plt.axis('off')\n        plt.title(f\"Score: {score:.2f}\")\n    \n    plt.figtext(0.1, 0.9 - i * 0.3, f\"Query: '{query}'\", fontsize=14)\n\nplt.tight_layout()\nplt.savefig(\"image_search_results.png\")\nplt.show()\n\n\n100%|██████████| 7/7 [00:00&lt;00:00, 64.81it/s]\n\n\n\n\n\n\n\n\n\n\n\nVQA TASK\nhttps://github.com/ArushiKumar11/DA312_VQA_Methods/tree/main/medical_vqa\nan implemented project of mine (earlier course project) that finetunes a CLIP Model trained on Medical data ( PubMedCLIP) on image scan dataset",
    "crumbs": [
      "About",
      "Posts",
      "Learning Transferable Visual Models From Natural Language Supervision (CLIP): A Deep Dive"
    ]
  },
  {
    "objectID": "posts/arushi-mmdp2.html#reflections",
    "href": "posts/arushi-mmdp2.html#reflections",
    "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP): A Deep Dive",
    "section": "Reflections",
    "text": "Reflections\n\nWhat Surprised Me\n\nEfficiency of Contrastive Learning: The paper’s demonstration that contrastive learning is 4x more efficient than bag-of-words prediction and 12x more efficient than transformer language modeling was eye-opening. This efficiency gap explains why previous approaches with similar ideas but different training objectives couldn’t scale effectively.\nRobustness to Distribution Shift: One of the most surprising findings was that zero-shot CLIP models significantly outperform supervised models in terms of robustness to distribution shifts as per the paper. This suggests that the typical approach of optimizing for a specific dataset might inherently lead to models that overfit to dataset-specific patterns rather than learning generalizable visual concepts.\nPrompt Engineering’s Impact: The significant performance improvements achieved through simple prompt engineering techniques (up to 5% on ImageNet) demonstrate how crucial the interface between the model and the task specification is. As the paper quotes - “Another issue we encountered is that it’s relatively rare in our pre-training dataset for the text paired with the image to be just a single word. Usually the text is a full sentence describing the image in some way. To help bridge this distribution gap, we found that using the prompt template “A photo of a label.” tobeagooddefault that helps specify the text is about the content of the image. This often improves performance over the baseline of using only the label text.”\nDataset Influence Over Architecture: The paper suggests that the dataset’s scale and quality had a much larger impact on performance than architectural choices. This challenges the common research focus on architecture design and suggests allocating more resources to data curation and scaling.\n\n\n\nScope for Improvement\n\nComputational Efficiency: Despite being more efficient than alternatives, CLIP still requires enormous computational resources. The paper estimates that a 1000x increase in compute would be needed for zero-shot CLIP to match state-of-the-art supervised models on all tasks. Developing more compute-efficient training methods would make this approach more accessible.\nFew-Shot Performance: While CLIP excels at zero-shot tasks, the transition to few-shot learning is somewhat counterintuitive. Adding just a few examples sometimes decreases performance relative to zero-shot predictions. Developing methods that better integrate prior knowledge (from zero-shot) with example-based learning could yield significant improvements.\nHandling Abstract and Systematic Tasks: CLIP struggles with abstract and systematic tasks like counting objects in an image. This suggests limitations in how well natural language supervision captures certain visual reasoning capabilities. Combining CLIP’s approach with methods specifically designed for reasoning tasks might address this gap.\nData Diversity and Bias: The web-scale training data inevitably contains biases present in internet text and images. While the paper acknowledges these issues, there’s significant room for improvement in developing methods to identify and mitigate these biases during training or inference.\nIntegration with Video Understanding: While CLIP shows strong performance on action recognition from single frames, extending its capabilities to understand temporal dynamics in videos would be a valuable improvement. This might involve adapting the contrastive learning objective to include temporal information.\nExplainability: Like many deep learning models, CLIP’s decision-making process lacks transparency. Developing methods to explain CLIP’s predictions would increase trust and enable more effective human-AI collaboration. Recent work on feature visualization and attribution methods for multimodal models represents a promising direction for making CLIP’s decisions more interpretable.",
    "crumbs": [
      "About",
      "Posts",
      "Learning Transferable Visual Models From Natural Language Supervision (CLIP): A Deep Dive"
    ]
  },
  {
    "objectID": "posts/arushi-mmdp2.html#references",
    "href": "posts/arushi-mmdp2.html#references",
    "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP): A Deep Dive",
    "section": "References",
    "text": "References\n\nPrimary Paper\n\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. arXiv preprint arXiv:2103.00020.\n\n\n\nResources Used for This Analysis\n\nOpenAI’s CLIP GitHub Repository: https://github.com/openai/CLIP\nCLIP Paper Implementation in PyTorch: https://github.com/openai/CLIP/tree/main/clip\nHugging Face’s CLIP Documentation: https://huggingface.co/docs/transformers/model_doc/clip\nBerkeley AI Research Blog (for reference on blog style): https://bair.berkeley.edu/blog/\nOther Blogs and Youtube Videos on CLIP Model for better understanding\nGithub Repos of Custom CLIP Model codes and codes on experiments on CLIP Model\nChatGPT for grammar correction, language refinement, and writing assistance (helping to improve clarity and structure of my own analysis)",
    "crumbs": [
      "About",
      "Posts",
      "Learning Transferable Visual Models From Natural Language Supervision (CLIP): A Deep Dive"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "Learning Transferable Visual Models From Natural Language Supervision (CLIP): A Deep Dive\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 8, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMay 5, 2025\n\n\nTristan O’Malley\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "About",
      "myblog"
    ]
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  }
]